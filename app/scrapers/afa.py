"""
AFA Stores Scraper - CLOUDFLARE BYPASS + CATEGORY-BASED SCRAPING
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î cloudscraper –¥–ª—è –æ–±—Ö–æ–¥—É Cloudflare + –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö –≤–∏—Ä–æ–±–Ω–∏–∫—ñ–≤
"""

import time
import logging
import json
from pathlib import Path
from typing import List, Dict, Optional, Set
from datetime import datetime
from ..modules.error_logger import ScraperErrorMixin

try:
    from curl_cffi import requests as curl_requests
    CURL_CFFI_AVAILABLE = True
except ImportError:
    CURL_CFFI_AVAILABLE = False
    try:
        import cloudscraper
        CLOUDSCRAPER_AVAILABLE = True
    except ImportError:
        CLOUDSCRAPER_AVAILABLE = False
        import requests
        logging.error("Neither curl_cffi nor cloudscraper installed! Install: pip install curl-cffi")

logger = logging.getLogger("afa")


class AFAScraper(ScraperErrorMixin):
    """Scraper –¥–ª—è afastores.com —á–µ—Ä–µ–∑ Shopify collections - category-based"""

    BASE_URL = "https://www.afastores.com"
    PRODUCTS_PER_PAGE = 30  # AFA –ø–æ–∫–∞–∑—É—î 30 —Ç–æ–≤–∞—Ä—ñ–≤ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫—É
    
    # Mapping –≤–∏—Ä–æ–±–Ω–∏–∫—ñ–≤ –¥–æ —ó—Ö slug –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
    MANUFACTURER_SLUGS = {
        "Steve Silver": "steve-silver",
        "Legacy Classic Furniture": "legacy-classic-furniture",
        "Legacy Classic Kids": "legacy-classic-kids",
        "Martin Furniture": "martin-furniture",
        "ACME Furniture": "acme-furniture",
        "Intercon Furniture": "intercon-furniture",
        "Westwood Design": "westwood-design"
    }

    def __init__(self, config: dict, error_logger=None):
        self.config = config
        self.error_logger = error_logger
        self.scraper_name = "AFAScraper"
        self.delay_min = config.get('delay_min', 1.0)
        self.delay_max = config.get('delay_max', 2.0)
        self.retry_attempts = config.get('retry_attempts', 3)
        self.timeout = config.get('timeout', 30)
        self.proxies = config.get('proxies', None)
        
        self.stats = {
            'total_products': 0,
            'unique_products': 0,
            'errors': 0,
            'manufacturers_processed': 0,
            'categories_processed': 0,
            'empty_categories': 0
        }

        # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –∑ JSON
        self.manufacturer_categories = self._load_categories()
        
        # Initialize session with best available method
        self.session_type = None
        self.impersonate = None

        if CURL_CFFI_AVAILABLE:
            self.session_type = 'curl_cffi'
            self.impersonate = 'chrome110'
            self.scraper = None
            logger.info(f"AFA Stores scraper initialized with curl_cffi (impersonate={self.impersonate})")

        elif CLOUDSCRAPER_AVAILABLE:
            import cloudscraper
            self.session_type = 'cloudscraper'
            self.scraper = cloudscraper.create_scraper(
                browser={
                    'browser': 'chrome',
                    'platform': 'windows',
                    'mobile': False
                },
                delay=10
            )

            self.scraper.headers.update({
                'Accept': 'application/json, text/plain, */*',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'Referer': 'https://www.afastores.com/',
                'Origin': 'https://www.afastores.com',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Sec-Fetch-Dest': 'empty',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Site': 'same-origin',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache'
            })

            logger.info("AFA Stores scraper initialized with cloudscraper")
            self._warm_up_session()

        else:
            import requests
            self.session_type = 'requests'
            self.scraper = requests.Session()
            self.scraper.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            logger.warning("AFA Stores scraper initialized with basic requests - will likely fail!")

    def _load_categories(self) -> dict:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –≤–∏—Ä–æ–±–Ω–∏–∫—ñ–≤ –∑ JSON —Ñ–∞–π–ª—É"""
        try:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ helper —Ñ—É–Ω–∫—Ü—ñ—é –∑ app.data
            # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ —è–∫—â–æ –∑–∞–ø—É—Å–∫–∞—î—Ç—å—Å—è —è–∫ –º–æ–¥—É–ª—å
            try:
                from ..data import load_manufacturer_categories
                categories = load_manufacturer_categories()
            except (ImportError, ValueError):
                # Fallback - –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞–ø—Ä—è–º—É
                from pathlib import Path
                import json
                
                data_dir = Path(__file__).parent.parent / "data"
                categories_file = data_dir / "manufacturer_categories.json"
                
                if not categories_file.exists():
                    logger.error(f"Categories file not found: {categories_file}")
                    return {}
                
                logger.info(f"Loading categories from: {categories_file}")
                with open(categories_file, 'r', encoding='utf-8') as f:
                    categories = json.load(f)
            
            logger.info(f"‚úì Loaded categories for {len(categories)} manufacturers")
            return categories
            
        except Exception as e:
            logger.error(f"Failed to load categories: {e}")
            return {}

    def _warm_up_session(self):
        """–û—Ç—Ä–∏–º—É—î –ø–æ—á–∞—Ç–∫–æ–≤—ñ cookies, –≤—ñ–¥–≤—ñ–¥—É—é—á–∏ –≥–æ–ª–æ–≤–Ω—É —Å—Ç–æ—Ä—ñ–Ω–∫—É"""
        if self.session_type == 'curl_cffi':
            logger.debug("Skipping warm-up for curl_cffi (not needed)")
            return

        try:
            logger.info("Warming up session by visiting homepage...")
            response = self.scraper.get(
                self.BASE_URL,
                timeout=self.timeout,
                proxies=self.proxies
            )
            response.raise_for_status()
            logger.info(f"Session warmed up. Cookies: {len(self.scraper.cookies)} items")
            time.sleep(2)
        except Exception as e:
            logger.warning(f"Failed to warm up session: {e}")

    def _random_delay(self):
        """–ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏"""
        import random
        time.sleep(random.uniform(self.delay_min, self.delay_max))

    def _fetch_category_products(self, category_slug: str, page: int) -> Optional[dict]:
        """
        –û—Ç—Ä–∏–º—É—î —Ç–æ–≤–∞—Ä–∏ –∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó —á–µ—Ä–µ–∑ Shopify JSON API

        Args:
            category_slug: Slug –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó (–Ω–∞–ø—Ä. "counter-stools-by-steve-silver")
            page: –ù–æ–º–µ—Ä —Å—Ç–æ—Ä—ñ–Ω–∫–∏

        Returns:
            JSON response –∞–±–æ None —É —Ä–∞–∑—ñ –ø–æ–º–∏–ª–∫–∏
        """
        url = f"{self.BASE_URL}/collections/{category_slug}/products.json"
        params = {'page': page}

        for attempt in range(self.retry_attempts):
            try:
                if self.session_type == 'curl_cffi':
                    response = curl_requests.get(
                        url,
                        params=params,
                        timeout=self.timeout,
                        impersonate=self.impersonate,
                        proxies=self.proxies
                    )
                else:
                    response = self.scraper.get(
                        url,
                        params=params,
                        timeout=self.timeout,
                        proxies=self.proxies
                    )

                response.raise_for_status()
                return response.json()

            except Exception as e:
                logger.warning(f"Request error (attempt {attempt+1}/{self.retry_attempts}): {e}")

                if attempt < self.retry_attempts - 1:
                    time.sleep(5)

        self.stats['errors'] += 1
        return None
    
    def _extract_products_from_json(self, json_data: dict, manufacturer_name: str) -> List[Dict[str, str]]:
        """
        –í–∏—Ç—è–≥—É—î —Ç–æ–≤–∞—Ä–∏ –∑ Shopify JSON API response

        Args:
            json_data: JSON response from /collections/.../products.json
            manufacturer_name: –ù–∞–∑–≤–∞ –≤–∏—Ä–æ–±–Ω–∏–∫–∞

        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä—ñ–≤
        """
        products = []

        try:
            for product in json_data.get('products', []):
                # –û–±—Ä–æ–±–∏—Ç–∏ –∫–æ–∂–µ–Ω –≤–∞—Ä—ñ–∞–Ω—Ç —Ç–æ–≤–∞—Ä—É
                for variant in product.get('variants', []):
                    sku = variant.get('sku', '').strip()

                    if not sku:
                        continue

                    # –ó–±–µ—Ä–µ–≥—Ç–∏ —Ç–æ–≤–∞—Ä
                    products.append({
                        'sku': sku,
                        'price': variant.get('price', ''),
                        'url': f"{self.BASE_URL}/products/{product.get('handle', '')}",
                        'title': product.get('title', ''),
                        'vendor': product.get('vendor', manufacturer_name),
                        'available': variant.get('available', False)
                    })

        except Exception as e:
            logger.error(f"Failed to extract products from JSON: {e}")

        return products
    
    def scrape_category(self, category_slug: str, manufacturer_name: str, seen_skus: Set[str]) -> List[Dict[str, str]]:
        """
        –ü–∞—Ä—Å–∏—Ç—å –≤—Å—ñ —Ç–æ–≤–∞—Ä–∏ –∑ –æ–¥–Ω—ñ—î—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó

        Args:
            category_slug: Slug –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó (–Ω–∞–ø—Ä. "counter-stools-by-steve-silver")
            manufacturer_name: –ù–∞–∑–≤–∞ –≤–∏—Ä–æ–±–Ω–∏–∫–∞ –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
            seen_skus: Set –¥–ª—è –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä—ñ–≤ –∑ —Ü—ñ—î—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
        """
        category_products = []
        page = 1
        
        while True:
            logger.debug(f"    Page {page}...")

            # –û—Ç—Ä–∏–º–∞—Ç–∏ JSON –∑ API
            json_data = self._fetch_category_products(category_slug, page)

            if not json_data:
                logger.debug(f"    No data on page {page}")
                break

            # –í–∏—Ç—è–≥—Ç–∏ products –∑ JSON
            page_products = self._extract_products_from_json(json_data, manufacturer_name)

            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–∏–π —Å–ø–∏—Å–æ–∫ - –∑—É–ø–∏–Ω–∫–∞
            if not page_products:
                logger.debug(f"    Empty products list on page {page} - stopping")
                break

            # –î–æ–¥–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ SKU
            new_count = 0
            for product in page_products:
                sku = product['sku']
                if sku not in seen_skus:
                    seen_skus.add(sku)
                    category_products.append(product)
                    new_count += 1

            logger.debug(f"    Page {page}: {len(page_products)} products, {new_count} new")

            # –Ø–∫—â–æ –º–µ–Ω—à–µ 30 —Ç–æ–≤–∞—Ä—ñ–≤ - —Ü–µ –æ—Å—Ç–∞–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∞
            if len(page_products) < self.PRODUCTS_PER_PAGE:
                logger.debug(f"    Got {len(page_products)} products (< {self.PRODUCTS_PER_PAGE}) - last page")
                break

            page += 1

            # –ó–∞—Ö–∏—Å—Ç –≤—ñ–¥ –Ω–µ—Å–∫—ñ–Ω—á–µ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª—É
            if page > 100:
                logger.warning(f"    Reached page limit (100) for category {category_slug}")
                break

            self._random_delay()

        return category_products
    
    def scrape_manufacturer(self, manufacturer_name: str, manufacturer_slug: str, 
                           seen_skus: Set[str]) -> List[Dict[str, str]]:
        """
        –ü–∞—Ä—Å–∏—Ç—å –≤—Å—ñ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –æ–¥–Ω–æ–≥–æ –≤–∏—Ä–æ–±–Ω–∏–∫–∞

        Args:
            manufacturer_name: –ù–∞–∑–≤–∞ –≤–∏—Ä–æ–±–Ω–∏–∫–∞ (–Ω–∞–ø—Ä. "Steve Silver")
            manufacturer_slug: Slug –≤–∏—Ä–æ–±–Ω–∏–∫–∞ –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
            seen_skus: Set –¥–ª—è –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä—ñ–≤ –≤—ñ–¥ —Ü—å–æ–≥–æ –≤–∏—Ä–æ–±–Ω–∏–∫–∞
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"Processing manufacturer: {manufacturer_name}")
        logger.info(f"{'='*60}")

        # –û—Ç—Ä–∏–º–∞—Ç–∏ —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –¥–ª—è —Ü—å–æ–≥–æ –≤–∏—Ä–æ–±–Ω–∏–∫–∞
        categories = self.manufacturer_categories.get(manufacturer_slug, [])
        
        if not categories:
            logger.warning(f"No categories found for {manufacturer_name} (slug: {manufacturer_slug})")
            return []

        logger.info(f"Found {len(categories)} categories for {manufacturer_name}")
        
        manufacturer_products = []
        start_time = datetime.now()
        
        for idx, category_slug in enumerate(categories, 1):
            logger.info(f"  [{idx}/{len(categories)}] Category: {category_slug}")
            
            # –ü–∞—Ä—Å–∏—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä—ñ—é
            category_products = self.scrape_category(category_slug, manufacturer_name, seen_skus)
            
            if category_products:
                manufacturer_products.extend(category_products)
                logger.info(f"    ‚úì Collected {len(category_products)} new products (total: {len(manufacturer_products)})")
            else:
                logger.info(f"    ‚äò Empty category")
                self.stats['empty_categories'] += 1
            
            self.stats['categories_processed'] += 1
            
            # Progress update –∫–æ–∂–Ω—ñ 10 –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
            if idx % 10 == 0:
                elapsed = (datetime.now() - start_time).total_seconds() / 60
                speed = idx / elapsed if elapsed > 0 else 0
                remaining = len(categories) - idx
                eta = remaining / speed if speed > 0 else 0
                
                logger.info(f"\n  üìä Progress: {idx}/{len(categories)} ({idx/len(categories)*100:.1f}%)")
                logger.info(f"     Products: {len(manufacturer_products)}")
                logger.info(f"     Speed: {speed:.1f} cat/min")
                logger.info(f"     ETA: {eta:.1f} min\n")
            
            # –ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏
            if idx < len(categories):
                self._random_delay()
        
        elapsed = (datetime.now() - start_time).total_seconds() / 60
        logger.info(f"\n‚úì Manufacturer {manufacturer_name} completed:")
        logger.info(f"  Categories processed: {len(categories)}")
        logger.info(f"  Products collected: {len(manufacturer_products)}")
        logger.info(f"  Time: {elapsed:.1f} minutes")
        
        self.stats['manufacturers_processed'] += 1
        
        return manufacturer_products
    
    def scrape_all_products(self) -> List[Dict[str, str]]:
        """–ü–∞—Ä—Å–∏—Ç—å –≤—Å—ñ —Ç–æ–≤–∞—Ä–∏ –≤—ñ–¥ –≤—Å—ñ—Ö –≤–∏—Ä–æ–±–Ω–∏–∫—ñ–≤"""
        
        all_products = []
        seen_skus: Set[str] = set()
        
        try:
            for manufacturer_name, manufacturer_slug in self.MANUFACTURER_SLUGS.items():
                try:
                    products = self.scrape_manufacturer(
                        manufacturer_name, 
                        manufacturer_slug, 
                        seen_skus
                    )
                    all_products.extend(products)
                    
                except Exception as e:
                    # ‚úÖ LOG ERROR
                    self.log_scraping_error(
                        error=e,
                        context={'manufacturer': manufacturer_name}
                    )
                    logger.error(f"Failed {manufacturer_name}: {e}")
                    continue
                
                time.sleep(3)
        
        except Exception as e:
            # ‚úÖ LOG GLOBAL ERROR
            self.log_scraping_error(error=e, context={'stage': 'main'})
            raise
        
        return all_products
    
    def get_stats(self) -> dict:
        """–ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        return self.stats.copy()

    def test_connection(self) -> dict:
        """–¢–µ—Å—Ç—É—î –∑'—î–¥–Ω–∞–Ω–Ω—è –∑ —Å–∞–π—Ç–æ–º –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"""
        results = {
            'homepage': False,
            'products_api': False,
            'ip_blocked': False,
            'cloudflare': False,
            'session_type': self.session_type,
            'details': []
        }

        # –¢–µ—Å—Ç 1: –î–æ—Å—Ç—É–ø –¥–æ –≥–æ–ª–æ–≤–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏
        try:
            logger.info("Testing homepage access...")

            if self.session_type == 'curl_cffi':
                resp = curl_requests.get(
                    self.BASE_URL,
                    timeout=self.timeout,
                    impersonate=self.impersonate,
                    proxies=self.proxies
                )
            else:
                resp = self.scraper.get(self.BASE_URL, timeout=self.timeout, proxies=self.proxies)

            results['homepage'] = resp.status_code == 200
            results['details'].append(f"Homepage: {resp.status_code}")

            if 'cloudflare' in resp.text.lower() or 'cf-ray' in resp.headers:
                results['cloudflare'] = True
                results['details'].append("Cloudflare detected")
        except Exception as e:
            results['details'].append(f"Homepage error: {e}")

        # –¢–µ—Å—Ç 2: Products JSON API (test category)
        try:
            logger.info("Testing products API...")
            
            # –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –ø–µ—Ä—à—É –∫–∞—Ç–µ–≥–æ—Ä—ñ—é –ø–µ—Ä—à–æ–≥–æ –≤–∏—Ä–æ–±–Ω–∏–∫–∞
            first_mfr_slug = list(self.MANUFACTURER_SLUGS.values())[0] if self.MANUFACTURER_SLUGS else None
            if first_mfr_slug and first_mfr_slug in self.manufacturer_categories:
                test_category = self.manufacturer_categories[first_mfr_slug][0]
                test_url = f"{self.BASE_URL}/collections/{test_category}/products.json"

                if self.session_type == 'curl_cffi':
                    resp = curl_requests.get(
                        test_url,
                        params={'page': 1},
                        timeout=self.timeout,
                        impersonate=self.impersonate,
                        proxies=self.proxies
                    )
                else:
                    resp = self.scraper.get(
                        test_url,
                        params={'page': 1},
                        timeout=self.timeout,
                        proxies=self.proxies
                    )

                results['products_api'] = resp.status_code == 200
                results['details'].append(f"Products API: {resp.status_code}")

                if resp.status_code == 403:
                    results['ip_blocked'] = True
                    results['details'].append("403 Forbidden - possible IP block")
        except Exception as e:
            results['details'].append(f"Products API error: {e}")
            if '403' in str(e):
                results['ip_blocked'] = True

        return results


def scrape_afa(config: dict) -> List[Dict[str, str]]:
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É AFA Stores"""
    scraper = AFAScraper(config)
    results = scraper.scrape_all_products()
    return results


if __name__ == "__main__":
    # –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(levelname)-8s | %(message)s',
        datefmt='%H:%M:%S'
    )

    if not CURL_CFFI_AVAILABLE and not CLOUDSCRAPER_AVAILABLE:
        print("\nERROR: Neither curl_cffi nor cloudscraper installed!")
        print("\nPreferred: pip install curl-cffi")
        print("Fallback: pip install cloudscraper")
        print("\nWithout one of these, AFA scraper will fail due to Cloudflare protection.\n")
        exit(1)

    if CURL_CFFI_AVAILABLE:
        print("\nUsing curl_cffi (best TLS fingerprint)")
    else:
        print("\nUsing cloudscraper (may not work on all systems)")
    
    test_config = {
        'delay_min': 1.0,
        'delay_max': 2.0,
        'retry_attempts': 3,
        'timeout': 30
    }
    
    print("\n" + "="*60)
    print("–¢–ï–°–¢ AFA STORES SCRAPER (CATEGORY-BASED)")
    print("="*60 + "\n")
    
    results = scrape_afa(test_config)
    
    print("\n" + "="*60)
    print(f"–†–ï–ó–£–õ–¨–¢–ê–¢: {len(results)} —Ç–æ–≤–∞—Ä—ñ–≤")
    print("="*60)
    
    if results:
        # –ü–æ–∫–∞–∑–∞—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤–∏—Ä–æ–±–Ω–∏–∫–∞—Ö
        vendors = {}
        for product in results:
            vendor = product['vendor']
            vendors[vendor] = vendors.get(vendor, 0) + 1
        
        print("\n–ü–æ –≤–∏—Ä–æ–±–Ω–∏–∫–∞—Ö:")
        for vendor, count in vendors.items():
            print(f"  {vendor}: {count} —Ç–æ–≤–∞—Ä—ñ–≤")
        
        print("\n–ü–µ—Ä—à—ñ 5 —Ç–æ–≤–∞—Ä—ñ–≤:")
        for i, product in enumerate(results[:5], 1):
            print(f"\n{i}. SKU: {product['sku']}")
            print(f"   Vendor: {product['vendor']}")
            print(f"   Price: ${product['price']}")
            if product.get('title'):
                print(f"   Title: {product['title'][:50]}...")
            if product.get('url'):
                print(f"   URL: {product['url'][:60]}...")
    else:
        print("\n‚ùå –ù–µ–º–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤")
        print("\n–ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ:")
        print("1. –ß–∏ —ñ—Å–Ω—É—î —Ñ–∞–π–ª manufacturer_categories.json")
        print("2. –ß–∏ Cloudflare –Ω–µ –±–ª–æ–∫—É—î –≤–∞—à IP")
        print("3. –õ–æ–≥–∏ –≤–∏—â–µ –¥–ª—è –¥–µ—Ç–∞–ª–µ–π")
