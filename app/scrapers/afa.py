"""
AFA Stores Scraper (Vendor Filter Method)
–ü–∞—Ä—Å–∏—Ç—å —Ü—ñ–Ω–∏ –∑ afastores.com —á–µ—Ä–µ–∑ Shopify JSON API
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î /products.json?vendor=VendorName - –ù–ê–ë–ê–ì–ê–¢–û –®–í–ò–î–®–ï!
"""

import requests
import time
import logging
from typing import List, Dict, Optional
from datetime import datetime

try:
    import cloudscraper
    CLOUDSCRAPER_AVAILABLE = True
except ImportError:
    CLOUDSCRAPER_AVAILABLE = False
    logging.warning("cloudscraper not available, using standard requests")

logger = logging.getLogger("afa")


class AFAScraper:
    """Scraper –¥–ª—è afastores.com —á–µ—Ä–µ–∑ Shopify API"""
    
    BASE_URL = "https://www.afastores.com"
    
    # –°–ø–∏—Å–æ–∫ vendors –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É
    DEFAULT_VENDORS = {
        "steve-silver": "Steve Silver",
        "martin-furniture": "Martin Furniture",
        "legacy-classic": "Legacy Classic",
        "coaster": "Coaster",
        "homelegance": "Homelegance",
        "lifestyle": "Lifestyle"
    }
    
    def __init__(self, config: dict):
        self.config = config
        self.delay_min = config.get('delay_min', 1.0)
        self.delay_max = config.get('delay_max', 2.0)
        self.retry_attempts = config.get('retry_attempts', 3)
        self.timeout = config.get('timeout', 30)
        self.test_mode = config.get('test_mode', False)
        
        # Vendors –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É (–º–æ–∂–Ω–∞ override –≤ config)
        self.vendors = config.get('vendors', self.DEFAULT_VENDORS)
        
        self.stats = {
            'total_products': 0,
            'unique_products': 0,
            'errors': 0,
            'vendors_processed': 0
        }
        
        # –í–∏–±—Ä–∞—Ç–∏ scraper
        if CLOUDSCRAPER_AVAILABLE:
            self.scraper = cloudscraper.create_scraper(
                browser={
                    'browser': 'chrome',
                    'platform': 'windows',
                    'mobile': False
                }
            )
            logger.info("AFA Stores scraper initialized (cloudscraper)")
        else:
            self.scraper = requests.Session()
            self.scraper.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'application/json'
            })
            logger.info("AFA Stores scraper initialized (requests)")
    
    def _random_delay(self):
        """–ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏"""
        import random
        time.sleep(random.uniform(self.delay_min, self.delay_max))
    
    def fetch_products_by_vendor(self, vendor_name: str, vendor_key: str, 
                                 seen_skus: set) -> List[Dict[str, str]]:
        """
        –ü–∞—Ä—Å–∏—Ç—å –≤—Å—ñ —Ç–æ–≤–∞—Ä–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ vendor —á–µ—Ä–µ–∑ Shopify API
        
        Args:
            vendor_name: –ù–∞–∑–≤–∞ vendor –≤ Shopify (–Ω–∞–ø—Ä. "Steve Silver")
            vendor_key: –ö–ª—é—á –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (–Ω–∞–ø—Ä. "steve-silver")
            seen_skus: Set –¥–ª—è –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä—ñ–≤ –∑ —Ü—å–æ–≥–æ vendor
        """
        logger.info(f"Processing vendor: {vendor_key} ({vendor_name})")
        
        products = []
        page = 1
        limit = 250  # –ú–∞–∫—Å–∏–º—É–º –¥–æ–∑–≤–æ–ª–µ–Ω–∏–π Shopify
        
        # TEST MODE: —Ç—ñ–ª—å–∫–∏ 1 —Å—Ç–æ—Ä—ñ–Ω–∫–∞
        max_pages = 1 if self.test_mode else 999
        
        while page <= max_pages:
            url = f"{self.BASE_URL}/products.json"
            params = {
                'vendor': vendor_name,
                'limit': limit,
                'page': page
            }
            
            try:
                logger.debug(f"  Fetching page {page}...")
                response = self.scraper.get(url, params=params, timeout=self.timeout)
                response.raise_for_status()
                
                data = response.json()
                page_products = data.get('products', [])
                
                if not page_products:
                    logger.info(f"  Page {page} is empty, stopping")
                    break
                
                # –û–±—Ä–æ–±–∏—Ç–∏ —Ç–æ–≤–∞—Ä–∏
                new_products = 0
                for product in page_products:
                    # –í–∏—Ç—è–≥—Ç–∏ –¥–∞–Ω—ñ –∑ –∫–æ–∂–Ω–æ–≥–æ –≤–∞—Ä—ñ–∞–Ω—Ç—É
                    for variant in product.get('variants', []):
                        sku = variant.get('sku', '').strip()
                        
                        if not sku or sku in seen_skus:
                            continue
                        
                        seen_skus.add(sku)
                        new_products += 1
                        
                        # –ó–±–µ—Ä–µ–≥—Ç–∏ —Ç–æ–≤–∞—Ä
                        products.append({
                            'sku': sku,
                            'price': variant.get('price', ''),
                            'url': f"{self.BASE_URL}/products/{product.get('handle')}",
                            'title': product.get('title', ''),
                            'vendor': product.get('vendor', ''),
                            'product_type': product.get('product_type', ''),
                            'available': variant.get('available', False),
                            'compare_at_price': variant.get('compare_at_price'),
                            'vendor_key': vendor_key
                        })
                
                logger.info(f"  Page {page}: {len(page_products)} products, {new_products} new variants")
                
                # –Ø–∫—â–æ –æ—Ç—Ä–∏–º–∞–ª–∏ –º–µ–Ω—à–µ –Ω—ñ–∂ limit - —Ü–µ –æ—Å—Ç–∞–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∞
                if len(page_products) < limit:
                    logger.info(f"  Received less than {limit} products, this is the last page")
                    break
                
                page += 1
                self._random_delay()
                
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 404:
                    logger.info(f"  Page {page} not found (404), stopping")
                    break
                else:
                    logger.error(f"HTTP error on page {page}: {e}")
                    self.stats['errors'] += 1
                    break
            except Exception as e:
                logger.error(f"Error on page {page} of {vendor_key}: {e}")
                self.stats['errors'] += 1
                break
        
        logger.info(f"Vendor {vendor_key}: collected {len(products)} products")
        self.stats['vendors_processed'] += 1
        
        return products
    
    def scrape_all_products(self) -> List[Dict[str, str]]:
        """–ü–∞—Ä—Å–∏—Ç—å –≤—Å—ñ —Ç–æ–≤–∞—Ä–∏ –∑ —É—Å—ñ—Ö vendors"""
        logger.info("="*60)
        logger.info("Starting AFA Stores scraping (Vendor Filter Method)")
        
        # TEST MODE: —Ç—ñ–ª—å–∫–∏ 1 vendor
        vendors_to_scrape = self.vendors
        if self.test_mode:
            # –í–∑—è—Ç–∏ –ø–µ—Ä—à–∏–π vendor
            first_key = list(self.vendors.keys())[0]
            vendors_to_scrape = {first_key: self.vendors[first_key]}
            logger.info(f"TEST MODE: Limited to 1 vendor ({first_key})")
        
        logger.info(f"Vendors: {len(vendors_to_scrape)}")
        logger.info("="*60)
        
        all_products = []
        seen_skus = set()
        
        for vendor_key, vendor_name in vendors_to_scrape.items():
            logger.info(f"[{self.stats['vendors_processed']+1}/{len(vendors_to_scrape)}] Processing: {vendor_key}")
            
            products = self.fetch_products_by_vendor(vendor_name, vendor_key, seen_skus)
            all_products.extend(products)
            
            self.stats['total_products'] = len(all_products)
            self.stats['unique_products'] = len(seen_skus)
            
            # –ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ vendors
            time.sleep(2)
        
        logger.info("="*60)
        logger.info(f"Completed: {len(all_products)} products from {len(seen_skus)} unique SKUs")
        logger.info(f"Vendors processed: {self.stats['vendors_processed']}")
        logger.info(f"Errors: {self.stats['errors']}")
        logger.info("="*60)
        
        return all_products
    
    def get_stats(self) -> dict:
        """–ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        return self.stats.copy()


def scrape_afa(config: dict) -> List[Dict[str, str]]:
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É AFA Stores"""
    scraper = AFAScraper(config)
    results = scraper.scrape_all_products()
    return results


# ============================================================================
# STANDALONE EXECUTION - –î–µ—Ç–∞–ª—å–Ω–µ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∑ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è–º —É —Ñ–∞–π–ª–∏
# ============================================================================

if __name__ == "__main__":
    import sys
    from pathlib import Path
    import json
    import csv
    
    # –î–æ–¥–∞—Ç–∏ project root –¥–æ path
    project_root = Path(__file__).parent.parent.parent
    sys.path.insert(0, str(project_root))
    
    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(name)-8s | %(levelname)-8s | %(message)s',
        datefmt='%H:%M:%S'
    )
    
    print("\n" + "="*70)
    print("AFA STORES SCRAPER - STANDALONE TEST")
    print("Vendor Filter Method (Shopify API)")
    print("="*70)
    print()
    
    if not CLOUDSCRAPER_AVAILABLE:
        print("‚ö†Ô∏è  WARNING: cloudscraper not installed!")
        print("For better Cloudflare bypass: pip install cloudscraper")
        print("Continuing with standard requests...\n")
    
    # –í–∏–±—ñ—Ä —Ä–µ–∂–∏–º—É
    print("Choose mode:")
    print("1. Test mode (1 vendor, 1 page) - –®–í–ò–î–ö–û ‚ö°")
    print("2. Full mode (all vendors, all pages) - –ü–û–í–Ü–õ–¨–ù–û ‚è±")
    choice = input("Enter choice [1/2, default=1]: ").strip() or "1"
    
    test_mode = (choice == "1")
    
    # –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
    test_config = {
        'delay_min': 1.0,
        'delay_max': 2.0,
        'retry_attempts': 3,
        'timeout': 30,
        'test_mode': test_mode
    }
    
    print()
    print("="*70)
    if test_mode:
        print("‚ö° TEST MODE: 1 vendor, 1 page (~30-60 seconds)")
    else:
        print("üî• FULL MODE: All vendors, all pages (~5-10 minutes)")
    print("="*70)
    print()
    
    # –ó–∞–ø—É—Å—Ç–∏—Ç–∏ scraper
    start_time = datetime.now()
    results = scrape_afa(test_config)
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    print()
    print("="*70)
    print("SCRAPING COMPLETED!")
    print("="*70)
    print(f"Duration: {duration:.1f} seconds ({duration/60:.1f} minutes)")
    print(f"Total products: {len(results)}")
    print()
    
    if results:
        # –ü–æ–∫–∞–∑–∞—Ç–∏ –ø—Ä–∏–∫–ª–∞–¥–∏
        print("="*70)
        print("SAMPLE PRODUCTS (First 5):")
        print("="*70)
        for i, product in enumerate(results[:5], 1):
            print(f"\n{i}. SKU: {product['sku']}")
            print(f"   Vendor: {product['vendor']} ({product['vendor_key']})")
            print(f"   Price: ${product['price']}")
            print(f"   Title: {product['title'][:60]}...")
            print(f"   URL: {product['url'][:60]}...")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ vendors
        print()
        print("="*70)
        print("BREAKDOWN BY VENDOR:")
        print("="*70)
        vendor_counts = {}
        for product in results:
            vendor = product['vendor_key']
            vendor_counts[vendor] = vendor_counts.get(vendor, 0) + 1
        
        for vendor, count in sorted(vendor_counts.items()):
            print(f"  {vendor}: {count} products")
        
        # –ó–±–µ—Ä–µ–≥—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        save = input("\nSave results to files? [y/N]: ").strip().lower()
        
        if save == 'y':
            output_dir = project_root / "output" / "afa"
            output_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # JSON
            json_path = output_dir / f"afa_products_{timestamp}.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"‚úì Saved JSON: {json_path}")
            
            # CSV
            csv_path = output_dir / f"afa_products_{timestamp}.csv"
            if results:
                keys = results[0].keys()
                with open(csv_path, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=keys)
                    writer.writeheader()
                    writer.writerows(results)
                print(f"‚úì Saved CSV: {csv_path}")
            
            print()
            print(f"Files saved to: {output_dir}")
    else:
        print("\n‚ùå No products found!")
        print("Possible reasons:")
        print("  - Cloudflare blocking (install cloudscraper)")
        print("  - Network issues")
        print("  - Vendor names changed")
    
    print()
    print("="*70)
