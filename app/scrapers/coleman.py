"""
Coleman Furniture Scraper
–ü–∞—Ä—Å–∏—Ç—å —Ü—ñ–Ω–∏ –∑ colemanfurniture.com —á–µ—Ä–µ–∑ REST API
"""

import requests
import time
import re
import logging
from typing import List, Dict, Optional
from datetime import datetime

logger = logging.getLogger("coleman")


class ColemanScraper:
    """Scraper –¥–ª—è colemanfurniture.com"""
    
    BASE_URL = "https://colemanfurniture.com"
    HEADER_API = "https://colemanfurniture.com/api/v2/header?storeid=1"
    
    def __init__(self, config: dict):
        self.config = config
        self.delay_min = config.get('delay_min', 0.5)
        self.delay_max = config.get('delay_max', 2.0)
        self.retry_attempts = config.get('retry_attempts', 3)
        self.timeout = config.get('timeout', 20)
        
        self.category_ids = {}
        self.stats = {
            'total_products': 0,
            'unique_products': 0,
            'errors': 0,
            'categories_processed': 0
        }
        
        logger.info("Coleman Furniture scraper initialized")
        self._load_category_ids()
    
    def _load_category_ids(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ ID –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –∑ API"""
        try:
            logger.info("Loading category IDs...")
            response = requests.get(self.HEADER_API, timeout=self.timeout)
            response.raise_for_status()
            data = response.json()
            
            header = data.get("data", data).get("header", {})
            menu_data = header.get("menu", [])
            
            self.category_ids = self._extract_ids(menu_data)
            logger.info(f"Loaded {len(self.category_ids)} categories")
            
        except Exception as e:
            logger.error(f"Failed to load category IDs: {e}")
            self.category_ids = {}
    
    def _extract_ids(self, menu: List[dict]) -> dict:
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –≤–∏—Ç—è–≥—É—î ID –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –∑ –º–µ–Ω—é"""
        ids = {}
        
        for item in menu:
            # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ Brands —Ç–∞ —ñ–Ω—à—ñ –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω—ñ —Ä–æ–∑–¥—ñ–ª–∏
            if item.get("title") == "Brands" or item.get("menuType") == "raw":
                continue
            
            # –û–±—Ä–æ–±–ª—è—î–º–æ –ø—ñ–¥–º–µ–Ω—é
            if "subMenu" in item and item["subMenu"]:
                for sub in item["subMenu"]:
                    route = sub.get("route", "")
                    match = re.search(r"id/(\d+)", route)
                    if match:
                        ids[sub.get("title")] = int(match.group(1))
                    
                    # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–æ–±–ª—è—î–º–æ –≤–∫–ª–∞–¥–µ–Ω—ñ –ø—ñ–¥–º–µ–Ω—é
                    ids.update(self._extract_ids([sub]))
        
        return ids
    
    def _random_delay(self):
        """–ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏"""
        import random
        time.sleep(random.uniform(self.delay_min, self.delay_max))
    
    def scrape_category(self, category_id: int, category_name: str, 
                       seen_skus: set) -> List[Dict[str, str]]:
        """–ü–∞—Ä—Å–∏—Ç—å –≤—Å—ñ —Ç–æ–≤–∞—Ä–∏ –∑ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó"""
        logger.info(f"Processing category: {category_name} (ID: {category_id})")
        
        products = []
        page = 1
        
        headers = {
            'Accept': 'application/json',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        while True:
            url = f"{self.BASE_URL}/catalog/category/view/id/{category_id}"
            params = {
                "order": "recommended",
                "p": page,
                "storeid": 1
            }
            
            try:
                response = requests.get(url, params=params, headers=headers, 
                                       timeout=self.timeout)
                response.raise_for_status()
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ Content-Type
                content_type = response.headers.get('Content-Type', '')
                if 'application/json' not in content_type:
                    logger.warning(f"Non-JSON response for {category_name} page {page}")
                    break
                
                data = response.json()
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ —Ç–æ–≤–∞—Ä—ñ–≤
                page_products = data.get("data", {}).get("content", {}).get("products", [])
                
                if not page_products:
                    logger.info(f"  Category {category_name}: collected {len(products)} products from {page-1} pages")
                    break
                
                # –í–∏—Ç—è–≥—É—î–º–æ –¥–∞–Ω—ñ —Ç–æ–≤–∞—Ä—ñ–≤
                for product in page_products:
                    sku = product.get("sku")
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª—ñ–∫–∞—Ç–∏
                    if not sku or sku in seen_skus:
                        continue
                    
                    seen_skus.add(sku)
                    
                    # –í–∏—Ç—è–≥—É—î–º–æ –¥–∞–Ω—ñ
                    manufacturer = product.get("manufacturer", {})
                    price_data = product.get("price", {})
                    
                    product_data = {
                        "sku": sku,
                        "manufacturer": manufacturer.get("title") if manufacturer else None,
                        "price": price_data.get("final"),
                        "url": product.get("url"),
                        "category_name": category_name,
                        "category_id": category_id
                    }
                    
                    products.append(product_data)
                
                logger.info(f"  Page {page}: found {len(page_products)} products (total: {len(products)})")
                
                page += 1
                self._random_delay()
                
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 404:
                    logger.info(f"  Category {category_name}: no more pages")
                    break
                else:
                    logger.error(f"HTTP error on page {page}: {e}")
                    self.stats['errors'] += 1
                    break
            except Exception as e:
                logger.error(f"Error on page {page} of {category_name}: {e}")
                self.stats['errors'] += 1
                break
        
        logger.info(f"Category {category_name}: collected {len(products)} products")
        self.stats['categories_processed'] += 1
        
        return products
    
    def scrape_all_products(self) -> List[Dict[str, str]]:
        logger.info("="*60)
        logger.info("Starting Coleman Furniture scraping")
        logger.info(f"Categories: {len(self.category_ids)}")
        logger.info("="*60)
        
        if not self.category_ids:
            logger.error("No categories loaded!")
            return []
        
        all_products = []
        seen_skus = set()
        start_time = datetime.now()
        last_progress_count = 0
        
        for idx, (category_name, category_id) in enumerate(self.category_ids.items(), 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"üìÇ [{idx}/{len(self.category_ids)}] {category_name}")
            logger.info(f"{'='*60}")
            
            products = self.scrape_category(category_id, category_name, seen_skus)
            all_products.extend(products)
            
            self.stats['total_products'] = len(all_products)
            self.stats['unique_products'] = len(seen_skus)
            
            # üÜï –ü–†–û–ì–†–ï–° –∫–æ–∂–Ω—ñ 1000 —Ç–æ–≤–∞—Ä—ñ–≤ (–∞–±–æ –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó)
            if len(all_products) - last_progress_count >= 1000 or idx % 5 == 0:
                elapsed = (datetime.now() - start_time).total_seconds() / 60
                speed = len(all_products) / elapsed if elapsed > 0 else 0
                
                # –û—Ü—ñ–Ω–∫–∞ —Å–∫—ñ–ª—å–∫–∏ —â–µ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
                categories_left = len(self.category_ids) - idx
                avg_per_category = len(all_products) / idx if idx > 0 else 0
                estimated_remaining = categories_left * avg_per_category
                eta = estimated_remaining / speed if speed > 0 else 0
                
                logger.info(f"\n{'üî•'*30}")
                logger.info(f"üìä COLEMAN PROGRESS")
                logger.info(f"{'üî•'*30}")
                logger.info(f"Categories: {idx}/{len(self.category_ids)} ({idx/len(self.category_ids)*100:.1f}%)")
                logger.info(f"Products: {len(all_products):,} ({len(seen_skus):,} unique)")
                logger.info(f"Speed: {speed:.1f} products/min")
                logger.info(f"Elapsed: {elapsed:.1f} min ({elapsed/60:.1f} hours)")
                logger.info(f"ETA: {eta:.1f} min (~{eta/60:.1f} hours)")
                logger.info(f"Errors: {self.stats['errors']}")
                logger.info(f"{'üî•'*30}\n")
                
                last_progress_count = len(all_products)
            
            # –ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏
            time.sleep(1)
        
        logger.info("="*60)
        logger.info(f"Completed: {len(all_products)} products from {len(seen_skus)} unique SKUs")
        logger.info(f"Categories processed: {self.stats['categories_processed']}")
        logger.info(f"Errors: {self.stats['errors']}")
        logger.info("="*60)
        
        return all_products

    def remove_duplicates(self, products: List[dict]) -> List[dict]:
        """–í–∏–¥–∞–ª—è—î –¥—É–±–ª—ñ–∫–∞—Ç–∏ –ø–æ SKU"""
        seen_skus = set()
        unique_products = []
        duplicates_count = 0
        
        for product in products:
            sku = product.get("sku")
            if sku and sku not in seen_skus:
                seen_skus.add(sku)
                unique_products.append(product)
            elif sku:
                duplicates_count += 1
        
        logger.info(f"Removed {duplicates_count} duplicates")
        return unique_products
    
    def get_stats(self) -> dict:
        """–ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        return self.stats.copy()


def scrape_coleman(config: dict) -> List[Dict[str, str]]:
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É Coleman Furniture"""
    scraper = ColemanScraper(config)
    results = scraper.scrape_all_products()
    return results


if __name__ == "__main__":
    # –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è
    import logging
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    
    test_config = {
        'delay_min': 0.5,
        'delay_max': 1.5,
        'retry_attempts': 3,
        'timeout': 20
    }
    
    print("\n" + "="*60)
    print("–¢–ï–°–¢ COLEMAN FURNITURE SCRAPER")
    print("="*60 + "\n")
    
    results = scrape_coleman(test_config)
    
    print("\n" + "="*60)
    print(f"–†–ï–ó–£–õ–¨–¢–ê–¢: {len(results)} —Ç–æ–≤–∞—Ä—ñ–≤")
    print("="*60)
    
    if results:
        print("\n–ü–µ—Ä—à—ñ 5 —Ç–æ–≤–∞—Ä—ñ–≤:")
        for i, product in enumerate(results[:5], 1):
            print(f"\n{i}. SKU: {product['sku']}")
            print(f"   Manufacturer: {product['manufacturer']}")
            print(f"   Price: ${product['price']}")
            print(f"   Category: {product['category_name']}")
            print(f"   URL: {product['url'][:60]}...")
