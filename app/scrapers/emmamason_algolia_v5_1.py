"""
Emma Mason Algolia API Scraper v5.1 - FIXED
âœ… ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¸Ğ¹ Ğ¾Ğ±Ñ…Ñ–Ğ´ pagination limit (1000 Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ–Ğ²)
âœ… Ğ Ğ¾Ğ·Ğ±Ğ¸Ñ‚Ñ‚Ñ Ğ²ĞµĞ»Ğ¸ĞºĞ¸Ñ… Ğ±Ñ€ĞµĞ½Ğ´Ñ–Ğ² Ñ‡ĞµÑ€ĞµĞ· collection_style facets
âœ… Ğ ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğµ Ñ€Ğ¾Ğ·Ğ±Ğ¸Ñ‚Ñ‚Ñ ÑĞºÑ‰Ğ¾ collection_style >1000
âœ… Ğ§Ğ¸ÑÑ‚Ğ¸Ğ¹ JSON Ğ±ĞµĞ· HTML parsing
âœ… 100% Ğ²ÑÑ–Ñ… Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ–Ğ² Ğ´Ğ»Ñ Ğ²ÑÑ–Ñ… Ğ±Ñ€ĞµĞ½Ğ´Ñ–Ğ²
âœ… Ğ’Ğ˜ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ: ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ° Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ° expired API key (400/403 errors)
"""

import time
import random
import logging
import sys
import json
from pathlib import Path
from typing import List, Dict, Optional, Set, Tuple
from datetime import datetime
from urllib.parse import quote

# Smart imports
try:
    from ..modules.error_logger import ScraperErrorMixin
except ImportError:
    project_root = Path(__file__).parent.parent.parent
    sys.path.insert(0, str(project_root))
    
    try:
        from app.modules.error_logger import ScraperErrorMixin
    except ImportError:
        class ScraperErrorMixin:
            def log_scraping_error(self, error, url=None, context=None):
                logger = logging.getLogger("emmamason_algolia")
                logger.error(f"Scraping error: {error}")

# requests
try:
    import requests
except ImportError:
    print("Error: requests library not installed!")
    print("Install: pip install requests")
    sys.exit(1)

logger = logging.getLogger("emmamason_algolia")


class AlgoliaAPIKeyExpired(Exception):
    """Exception ĞºĞ¾Ğ»Ğ¸ Algolia API key expired Ğ°Ğ±Ğ¾ invalid"""
    pass


class EmmaMasonAlgoliaScraperV5_1(ScraperErrorMixin):
    """
    Scraper Ğ´Ğ»Ñ Emma Mason Ñ‡ĞµÑ€ĞµĞ· Algolia Search API
    v5.1: ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¸Ğ¹ Ğ¾Ğ±Ñ…Ñ–Ğ´ pagination limit Ñ‡ĞµÑ€ĞµĞ· facets
    """
    
    # Algolia API Configuration
    ALGOLIA_URL = "https://ctz7lv7pje-dsn.algolia.net/1/indexes/*/queries"
    ALGOLIA_APP_ID = "CTZ7LV7PJE"
    ALGOLIA_API_KEY = "MjgzMmViMDZiYmYzZTA4YTY2NjhkYjNkMjAyMzUxYmE5Y2Y4MzYwYzZmMmRiODU0NzQxMmY0YWFmNDM3YjllZHRhZ0ZpbHRlcnM9JnZhbGlkVW50aWw9MTc2OTUxMTczNw=="
    INDEX_NAME = "magento2_emmamason_products"
    
    # Pagination limit
    PAGINATION_LIMIT = 1000
    
    # Brands to scrape
    BRANDS = [
        "Intercon Furniture",
        "ACME",
        "Aspenhome",
        "Steve Silver",
        "Westwood Design",
        "Legacy Classic",
        "Legacy Classic Kids",
    ]
    
    def __init__(self, config: dict, error_logger=None):
        self.config = config
        self.error_logger = error_logger
        self.scraper_name = "EmmaMasonAlgoliaScraper"
        
        self.delay_min = config.get('delay_min', 0.5)
        self.delay_max = config.get('delay_max', 1.5)
        self.retry_attempts = config.get('retry_attempts', 3)
        self.timeout = config.get('timeout', 30)
        self.hits_per_page = config.get('hits_per_page', 1000)
        
        logger.info("="*60)
        logger.info("Emma Mason Algolia API Scraper v5.1 (Smart Pagination)")
        logger.info("="*60)
        logger.info(f"Feature: Auto-split via facets to bypass 1000 limit")
    
    def _random_delay(self):
        time.sleep(random.uniform(self.delay_min, self.delay_max))
    
    def _build_facet_filters(self, filters: List[Tuple[str, str]]) -> str:
        """
        ĞŸĞ¾Ğ±ÑƒĞ´ÑƒĞ²Ğ°Ñ‚Ğ¸ facetFilters
        
        Args:
            filters: [(facet_name, value), ...]
        
        Returns:
            URL-encoded facetFilters string
        """
        # Format: [["brand:ACME","collection_style:Contemporary"]]
        filter_strings = [f'"{name}:{value}"' for name, value in filters]
        return f'[[{",".join(filter_strings)}]]'
    
    def _build_params(self, filters: List[Tuple[str, str]], page: int = 0, 
                      facets: List[str] = None, hits: int = None) -> str:
        """ĞŸĞ¾Ğ±ÑƒĞ´ÑƒĞ²Ğ°Ñ‚Ğ¸ URL params Ğ´Ğ»Ñ Algolia"""
        
        facet_filters = self._build_facet_filters(filters)
        
        params = {
            'facetFilters': facet_filters,
            'hitsPerPage': str(hits or self.hits_per_page),
            'page': str(page),
            'numericFilters': '["visibility_search=1"]',
            'highlightPreTag': '__ais-highlight__',
            'highlightPostTag': '__/ais-highlight__',
        }
        
        if facets:
            params['facets'] = str(facets).replace("'", '"')
        
        return '&'.join([f"{k}={quote(str(v))}" for k, v in params.items()])
    
    def _build_params_with_price(self, filters: List[Tuple[str, str]], 
                                  min_price: float, max_price: float, 
                                  page: int = 0, hits: int = None) -> str:
        """
        ĞŸĞ¾Ğ±ÑƒĞ´ÑƒĞ²Ğ°Ñ‚Ğ¸ params Ğ· price range
        
        Args:
            filters: Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ– Ñ„Ñ–Ğ»ÑŒÑ‚Ñ€Ğ¸
            min_price: ĞœÑ–Ğ½Ñ–Ğ¼Ğ°Ğ»ÑŒĞ½Ğ° Ñ†Ñ–Ğ½Ğ°
            max_price: ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ° Ñ†Ñ–Ğ½Ğ°
        """
        facet_filters = self._build_facet_filters(filters)
        
        # Ğ”Ğ¾Ğ´Ğ°Ñ‚Ğ¸ price range Ğ´Ğ¾ numericFilters
        numeric_filters = [
            "visibility_search=1",
            f"price.USD.default>={min_price}",
            f"price.USD.default<{max_price}"
        ]
        numeric_filters_str = str(numeric_filters).replace("'", '"')
        
        params = {
            'facetFilters': facet_filters,
            'hitsPerPage': str(hits or self.hits_per_page),
            'page': str(page),
            'numericFilters': numeric_filters_str,
            'highlightPreTag': '__ais-highlight__',
            'highlightPostTag': '__/ais-highlight__',
        }
        
        return '&'.join([f"{k}={quote(str(v))}" for k, v in params.items()])
    
    def _fetch_algolia(self, params_string: str) -> Optional[Dict]:
        """
        Ğ’Ğ¸ĞºĞ¾Ğ½Ğ°Ñ‚Ğ¸ Ğ·Ğ°Ğ¿Ğ¸Ñ‚ Ğ´Ğ¾ Algolia API
        
        Returns:
            Response data Ğ°Ğ±Ğ¾ None
        
        Raises:
            AlgoliaAPIKeyExpired: Ğ¯ĞºÑ‰Ğ¾ API key expired (400/403)
        """
        for attempt in range(1, self.retry_attempts + 1):
            try:
                headers = {
                    "accept": "*/*",
                    "content-type": "application/x-www-form-urlencoded",
                    "x-algolia-api-key": self.ALGOLIA_API_KEY,
                    "x-algolia-application-id": self.ALGOLIA_APP_ID,
                    "origin": "https://emmamason.com",
                    "referer": "https://emmamason.com/",
                    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                }
                
                payload = {
                    "requests": [
                        {"indexName": self.INDEX_NAME, "params": params_string}
                    ]
                }
                
                response = requests.post(
                    self.ALGOLIA_URL, json=payload,
                    headers=headers, timeout=self.timeout
                )
                
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # âœ… ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ: ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€ĞºĞ° Ğ½Ğ° expired key
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                if response.status_code == 200:
                    return response.json()
                
                elif response.status_code in [400, 403]:
                    # API key expired Ğ°Ğ±Ğ¾ invalid
                    logger.error(f"âŒ API key issue - Status {response.status_code}")
                    logger.error(f"Response: {response.text[:200]}")
                    
                    # Ğ’Ğ¸ĞºĞ¸Ğ½ÑƒÑ‚Ğ¸ exception Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ refresh
                    raise AlgoliaAPIKeyExpired(
                        f"API key expired or invalid (status {response.status_code})"
                    )
                
                else:
                    logger.warning(f"Status {response.status_code} (attempt {attempt})")
                    if attempt < self.retry_attempts:
                        time.sleep(2)
            
            except AlgoliaAPIKeyExpired:
                # ĞŸĞµÑ€ĞµĞ´Ğ°Ñ‚Ğ¸ exception Ğ²Ğ³Ğ¾Ñ€Ñƒ
                raise
            
            except Exception as e:
                error_msg = str(e).lower()
                
                # ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€Ğ¸Ñ‚Ğ¸ Ñ‡Ğ¸ Ñ†Ğµ Ğ¼Ğ¾Ğ¶Ğµ Ğ±ÑƒÑ‚Ğ¸ expired key
                if any(keyword in error_msg for keyword in [
                    'forbidden', 'unauthorized', 'invalid api key'
                ]):
                    raise AlgoliaAPIKeyExpired(f"API authentication error: {e}")
                
                logger.error(f"Request error (attempt {attempt}): {e}")
                if attempt < self.retry_attempts:
                    time.sleep(2)
        
        # Ğ¯ĞºÑ‰Ğ¾ Ğ²ÑÑ– ÑĞ¿Ñ€Ğ¾Ğ±Ğ¸ Ğ½ĞµĞ²Ğ´Ğ°Ğ»Ñ–
        logger.error("All retry attempts failed")
        return None
    
    def _get_facets(self, filters: List[Tuple[str, str]], 
                    facet_name: str = "collection_style") -> Dict[str, int]:
        """
        ĞÑ‚Ñ€Ğ¸Ğ¼Ğ°Ñ‚Ğ¸ facets Ğ´Ğ»Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñƒ Ñ„Ñ–Ğ»ÑŒÑ‚Ñ€Ñ–Ğ²
        
        Args:
            filters: ĞŸĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ– Ñ„Ñ–Ğ»ÑŒÑ‚Ñ€Ğ¸
            facet_name: Ğ¯ĞºĞ¸Ğ¹ facet Ğ¾Ñ‚Ñ€Ğ¸Ğ¼Ğ°Ñ‚Ğ¸
        
        Returns:
            {facet_value: count}
        """
        params = self._build_params(
            filters=filters,
            facets=[facet_name],
            hits=0  # ĞĞµ Ñ‚Ñ€ĞµĞ±Ğ° hits, Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ facets
        )
        
        data = self._fetch_algolia(params)
        
        if not data:
            return {}
        
        facets = data['results'][0].get('facets', {})
        return facets.get(facet_name, {})
    
    def _parse_hits(self, hits: List[Dict], brand: str) -> List[Dict]:
        """ĞŸĞ°Ñ€ÑĞ¸Ñ‚Ğ¸ hits Ğ² products"""
        products = []
        
        for hit in hits:
            try:
                product = {
                    'id': hit.get('objectID'),
                    'sku': hit.get('sku'),
                    'name': hit.get('name'),
                    'url': hit.get('url'),
                    'brand': brand,
                    'price': self._extract_price(hit),
                    'in_stock': hit.get('in_stock'),
                    'categories': hit.get('categories', []),
                    'collection_style': hit.get('collection_style'),
                    'scraped_at': datetime.now().isoformat()
                }
                
                products.append(product)
            
            except Exception as e:
                logger.debug(f"Failed to parse hit {hit.get('objectID')}: {e}")
                continue
        
        return products
    
    def _extract_price(self, hit: Dict) -> Optional[str]:
        """Ğ’Ğ¸Ñ‚ÑĞ³Ñ‚Ğ¸ Ñ†Ñ–Ğ½Ñƒ Ğ· hit"""
        try:
            price_data = hit.get('price', {})
            
            if isinstance(price_data, dict):
                usd = price_data.get('USD', {})
                
                if isinstance(usd, dict):
                    default_price = usd.get('default')
                    
                    if default_price is not None:
                        return str(default_price)
            
            return None
        
        except Exception as e:
            logger.debug(f"Price extraction error: {e}")
            return None
    
    def _split_price_range(self, min_price: float, max_price: float) -> List[Tuple[float, float]]:
        """
        Ğ Ğ¾Ğ·Ğ±Ğ¸Ñ‚Ğ¸ price range Ğ½Ğ° Ğ¼ĞµĞ½ÑˆÑ– Ğ´Ñ–Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ¸
        """
        # Ğ¯ĞºÑ‰Ğ¾ Ğ´Ñ–Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ Ğ¼Ğ°Ğ»Ğ¸Ğ¹ - Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ñ€Ğ¾Ğ·Ğ´Ñ–Ğ»Ğ¸Ñ‚Ğ¸ Ğ½Ğ°Ğ²Ğ¿Ñ–Ğ»
        if max_price - min_price <= 500:
            mid = (min_price + max_price) / 2
            return [(min_price, mid), (mid, max_price)]
        
        # Ğ†Ğ½Ğ°ĞºÑˆĞµ - Ñ€Ğ¾Ğ·Ğ±Ğ¸Ñ‚Ğ¸ Ğ½Ğ° 5 Ñ‡Ğ°ÑÑ‚Ğ¸Ğ½
        step = (max_price - min_price) / 5
        ranges = []
        
        for i in range(5):
            start = min_price + (i * step)
            end = min_price + ((i + 1) * step)
            ranges.append((start, end))
        
        return ranges
    
    def _scrape_price_range(self, filters: List[Tuple[str, str]], 
                           brand: str, seen_ids: Set[str],
                           min_price: float, max_price: float) -> List[Dict]:
        """
        Scrape Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¸ Ğ² price range
        """
        all_products = []
        page = 0
        
        while True:
            params = self._build_params_with_price(
                filters=filters,
                min_price=min_price,
                max_price=max_price,
                page=page
            )
            
            data = self._fetch_algolia(params)
            
            if not data:
                break
            
            hits = data['results'][0].get('hits', [])
            
            if not hits:
                break
            
            # Parse
            products = self._parse_hits(hits, brand)
            
            # Ğ”ĞµĞ´ÑƒĞ¿Ğ»Ñ–ĞºĞ°Ñ†Ñ–Ñ
            new_count = 0
            for product in products:
                if product['id'] not in seen_ids:
                    seen_ids.add(product['id'])
                    all_products.append(product)
                    new_count += 1
            
            logger.debug(f"        Page {page}: {new_count} new")
            
            # ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€Ğ¸Ñ‚Ğ¸ Ñ‡Ğ¸ Ñ” Ñ‰Ğµ
            if len(hits) < self.hits_per_page:
                break
            
            page += 1
            self._random_delay()
        
        return all_products
    
    def _scrape_with_filters(self, filters: List[Tuple[str, str]], 
                            brand: str, seen_ids: Set[str], 
                            depth: int = 0) -> List[Dict]:
        """
        Ğ Ğ¾Ğ·ÑƒĞ¼Ğ½Ğ¸Ğ¹ scraping Ğ· Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¸Ğ¼ Ñ€Ğ¾Ğ·Ğ±Ğ¸Ñ‚Ñ‚ÑĞ¼
        
        Args:
            filters: ĞŸĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ– Ñ„Ñ–Ğ»ÑŒÑ‚Ñ€Ğ¸
            brand: ĞĞ°Ğ·Ğ²Ğ° Ğ±Ñ€ĞµĞ½Ğ´Ñƒ
            seen_ids: Set Ğ´Ğ»Ñ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ñ–ĞºĞ°Ñ†Ñ–Ñ—
            depth: Ğ“Ğ»Ğ¸Ğ±Ğ¸Ğ½Ğ° Ñ€ĞµĞºÑƒÑ€ÑÑ–Ñ— (Ğ·Ğ°Ñ…Ğ¸ÑÑ‚ Ğ²Ñ–Ğ´ Ğ½ĞµÑĞºÑ–Ğ½Ñ‡ĞµĞ½Ğ½Ğ¾Ñ— Ñ€ĞµĞºÑƒÑ€ÑÑ–Ñ—)
        
        Returns:
            Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ–Ğ²
        """
        # âœ… Ğ—ĞĞ¥Ğ˜Ğ¡Ğ¢: ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ° Ğ³Ğ»Ğ¸Ğ±Ğ¸Ğ½Ğ° Ñ€ĞµĞºÑƒÑ€ÑÑ–Ñ—
        MAX_DEPTH = 3
        if depth >= MAX_DEPTH:
            logger.warning(f"  âš ï¸  Max recursion depth reached, using simple scrape")
            return self._scrape_simple(filters, brand, seen_ids)
        
        # ĞšÑ€Ğ¾Ğº 1: ĞÑ‚Ñ€Ğ¸Ğ¼Ğ°Ñ‚Ğ¸ Ğ·Ğ°Ğ³Ğ°Ğ»ÑŒĞ½Ñƒ ĞºÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ
        params = self._build_params(filters=filters, hits=0)
        data = self._fetch_algolia(params)
        
        if not data:
            logger.error(f"  Failed to fetch for filters: {filters}")
            return []
        
        nb_hits = data['results'][0].get('nbHits', 0)
        
        filter_str = " + ".join([f"{n}={v}" for n, v in filters])
        logger.debug(f"  [Depth {depth}] Filters: {filter_str} â†’ {nb_hits} hits")
        
        # ĞšÑ€Ğ¾Ğº 2: Ğ¯ĞºÑ‰Ğ¾ â‰¤1000 - Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ğ¸Ñ‚Ğ¸
        if nb_hits <= self.PAGINATION_LIMIT:
            return self._scrape_simple(filters, brand, seen_ids)
        
        # ĞšÑ€Ğ¾Ğº 3: Ğ¯ĞºÑ‰Ğ¾ >1000 - Ñ€Ğ¾Ğ·Ğ±Ğ¸Ñ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· PRICE RANGES (Ğ½Ğµ collection_style!)
        logger.info(f"  ğŸ”„ Splitting {nb_hits} hits via price ranges (depth {depth})...")
        
        # âœ… Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ²Ğ°Ñ‚Ğ¸ price ranges Ğ·Ğ°Ğ¼Ñ–ÑÑ‚ÑŒ collection_style
        # Ğ¦Ğµ Ğ·Ğ°Ğ²Ğ¶Ğ´Ğ¸ Ğ¿Ñ€Ğ°Ñ†ÑÑ”, Ğ±Ğ¾ Ñ†Ñ–Ğ½Ğ¸ Ñ€Ñ–Ğ·Ğ½Ñ– Ğ´Ğ»Ñ ĞºĞ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñƒ
        price_ranges = [
            (0, 200),
            (200, 500),
            (500, 1000),
            (1000, 2000),
            (2000, 10000)
        ]
        
        all_products = []
        
        for min_price, max_price in price_ranges:
            logger.info(f"    â€¢ Price ${min_price}-${max_price}")
            
            # ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€Ğ¸Ñ‚Ğ¸ ÑĞºÑ–Ğ»ÑŒĞºĞ¸ hits Ğ² Ñ†ÑŒĞ¾Ğ¼Ñƒ range
            price_params = self._build_params_with_price(
                filters=filters,
                min_price=min_price,
                max_price=max_price,
                hits=0  # Ğ¢Ñ–Ğ»ÑŒĞºĞ¸ count
            )
            
            data = self._fetch_algolia(price_params)
            
            if not data:
                continue
            
            range_hits = data['results'][0].get('nbHits', 0)
            
            if range_hits == 0:
                logger.debug(f"      â†’ 0 hits, skipping")
                continue
            
            logger.debug(f"      â†’ {range_hits} hits in this range")
            
            # Ğ¯ĞºÑ‰Ğ¾ price range >1000 - Ñ€Ğ¾Ğ·Ğ±Ğ¸Ñ‚Ğ¸ Ğ½Ğ° Ğ¼ĞµĞ½ÑˆÑ– Ğ´Ñ–Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ¸
            if range_hits > self.PAGINATION_LIMIT:
                logger.info(f"      ğŸ”„ Splitting ${min_price}-${max_price} further...")
                sub_ranges = self._split_price_range(min_price, max_price)
                
                for sub_min, sub_max in sub_ranges:
                    products = self._scrape_price_range(
                        filters=filters,
                        brand=brand,
                        seen_ids=seen_ids,
                        min_price=sub_min,
                        max_price=sub_max
                    )
                    all_products.extend(products)
                    self._random_delay()
            else:
                # ĞŸÑ€Ğ¾ÑÑ‚Ğ¸Ğ¹ scraping Ğ´Ğ»Ñ Ñ†ÑŒĞ¾Ğ³Ğ¾ range
                products = self._scrape_price_range(
                    filters=filters,
                    brand=brand,
                    seen_ids=seen_ids,
                    min_price=min_price,
                    max_price=max_price
                )
                all_products.extend(products)
            
            self._random_delay()
        
        return all_products
    
    def _scrape_simple(self, filters: List[Tuple[str, str]], 
                      brand: str, seen_ids: Set[str]) -> List[Dict]:
        """
        ĞŸÑ€Ğ¾ÑÑ‚Ğ¸Ğ¹ scraping (â‰¤1000 Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ–Ğ²) Ğ· Ğ¿Ğ°Ğ³Ñ–Ğ½Ğ°Ñ†Ñ–Ñ”Ñ
        """
        all_products = []
        page = 0
        
        while True:
            params = self._build_params(filters=filters, page=page)
            data = self._fetch_algolia(params)
            
            if not data:
                logger.error(f"  Failed page {page}")
                break
            
            hits = data['results'][0].get('hits', [])
            
            if not hits:
                break
            
            # Parse
            products = self._parse_hits(hits, brand)
            
            # Ğ”ĞµĞ´ÑƒĞ¿Ğ»Ñ–ĞºĞ°Ñ†Ñ–Ñ
            new_count = 0
            for product in products:
                if product['id'] not in seen_ids:
                    seen_ids.add(product['id'])
                    all_products.append(product)
                    new_count += 1
            
            logger.debug(f"    Page {page}: {new_count} new ({len(all_products)} total)")
            
            # ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€Ğ¸Ñ‚Ğ¸ Ñ‡Ğ¸ Ñ” Ñ‰Ğµ
            if len(hits) < self.hits_per_page:
                break
            
            page += 1
            self._random_delay()
        
        return all_products
    
    def scrape_brand(self, brand: str, seen_ids: Set[str]) -> List[Dict]:
        """
        Scrape Ğ±Ñ€ĞµĞ½Ğ´ Ğ· Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¸Ğ¼ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ¾Ğ¼ pagination limit
        """
        logger.info(f"\nProcessing brand: {brand}")
        
        # ĞŸĞ¾Ñ‡Ğ°Ñ‚ĞºĞ¾Ğ²Ñ– Ñ„Ñ–Ğ»ÑŒÑ‚Ñ€Ğ¸: Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ğ±Ñ€ĞµĞ½Ğ´
        filters = [("brand", brand)]
        
        # Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ°Ñ‚Ğ¸ Ñ€Ğ¾Ğ·ÑƒĞ¼Ğ½Ğ¸Ğ¹ scraping (depth=0 Ğ¿Ğ¾Ñ‡Ğ°Ñ‚ĞºĞ¾Ğ²Ğ¾)
        products = self._scrape_with_filters(filters, brand, seen_ids, depth=0)
        
        logger.info(f"âœ“ Brand {brand}: {len(products)} products")
        
        return products
    
    def scrape_all_brands(self) -> List[Dict]:
        """
        Scrape Ğ²ÑÑ–Ñ… Ğ±Ñ€ĞµĞ½Ğ´Ñ–Ğ²
        
        Raises:
            AlgoliaAPIKeyExpired: Ğ¯ĞºÑ‰Ğ¾ API key expired
        """
        all_products = []
        seen_ids = set()
        
        try:
            for idx, brand in enumerate(self.BRANDS, 1):
                try:
                    logger.info(f"\n[{idx}/{len(self.BRANDS)}] Starting: {brand}")
                    
                    products = self.scrape_brand(brand, seen_ids)
                    all_products.extend(products)
                    
                except AlgoliaAPIKeyExpired:
                    # ĞŸĞµÑ€ĞµĞ´Ğ°Ñ‚Ğ¸ exception Ğ²Ğ³Ğ¾Ñ€Ñƒ Ğ´Ğ»Ñ auto-refresh
                    logger.error(f"API key expired while processing {brand}")
                    raise
                
                except Exception as e:
                    self.log_scraping_error(error=e, context={'brand': brand})
                    logger.error(f"Failed {brand}: {e}")
                    continue
                
                time.sleep(random.uniform(1, 2))
        
        except AlgoliaAPIKeyExpired:
            # ĞŸĞµÑ€ĞµĞ´Ğ°Ñ‚Ğ¸ Ğ²Ğ³Ğ¾Ñ€Ñƒ
            raise
        
        except Exception as e:
            self.log_scraping_error(error=e, context={'stage': 'main'})
            raise
        
        # Stats
        logger.info("\n" + "="*60)
        logger.info("SCRAPING COMPLETED")
        logger.info("="*60)
        logger.info(f"Total products: {len(all_products)}")
        logger.info(f"Unique IDs: {len(seen_ids)}")
        
        brands_stats = {}
        for p in all_products:
            brand = p['brand']
            brands_stats[brand] = brands_stats.get(brand, 0) + 1
        
        logger.info("\nBy brand:")
        for brand, count in brands_stats.items():
            logger.info(f"  {brand}: {count}")
        
        logger.info("="*60)
        
        return all_products


def scrape_emmamason_algolia(config: dict, error_logger=None) -> List[Dict]:
    """Main function Ğ´Ğ»Ñ v5.1"""
    scraper = EmmaMasonAlgoliaScraperV5_1(config, error_logger)
    return scraper.scrape_all_brands()


# if __name__ == "__main__":
#     import logging
#     logging.basicConfig(
#         level=logging.INFO,
#         format='%(asctime)s | %(levelname)-8s | %(message)s',
#         datefmt='%H:%M:%S'
#     )
    
#     config = {
#         'delay_min': 0.5,
#         'delay_max': 1.5,
#         'retry_attempts': 3,
#         'timeout': 30,
#         'hits_per_page': 1000
#     }
    
#     print("\n" + "="*60)
#     print("Ğ¢Ğ•Ğ¡Ğ¢ ALGOLIA SCRAPER v5.1 (Smart Pagination)")
#     print("="*60 + "\n")
    
#     results = scrape_emmamason_algolia(config)
    
#     print(f"\nâœ… Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢: {len(results)} products")
