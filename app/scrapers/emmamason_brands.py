"""
Emma Mason Brands Scraper - PRODUCTION INTEGRATED VERSION
100% —Å—É–º—ñ—Å–Ω–∏–π –∑ app/main.py —Ç–∞ app/modules/google_sheets.py

–ú–ï–•–ê–ù–Ü–ó–ú:
1. –ó–±–∏—Ä–∞—î —Ç–æ–≤–∞—Ä–∏ –∑ –±—Ä–µ–Ω–¥—ñ–≤ ‚Üí —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ {id, url, price}
2. main.py –≤–∏–∫–ª–∏–∫–∞—î scrape_emmamason_brands(config)
3. google_sheets.batch_update_emma_mason() —à—É–∫–∞—î –ø–æ URL —Ç–∞ –∑–∞–ø–∏—Å—É—î
"""

import time
import random
import logging
from typing import List, Dict, Optional, Set
from datetime import datetime
from bs4 import BeautifulSoup

# –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ curl_cffi
try:
    from curl_cffi import requests
    CURL_CFFI_AVAILABLE = True
except ImportError:
    import requests
    CURL_CFFI_AVAILABLE = False
    logging.warning("‚ö†Ô∏è curl_cffi not found, using standard requests")

logger = logging.getLogger("emmamason_brands")


class EmmaMasonBrandsScraper:
    """Scraper –¥–ª—è emmamason.com - –∑–±–∏—Ä–∞—î –ø–æ –±—Ä–µ–Ω–¥–∞—Ö"""
    
    BASE_URL = "https://emmamason.com"
    
    # –¶—ñ–ª—å–æ–≤—ñ –±—Ä–µ–Ω–¥–∏ (–∑ –≤–∞—à–æ–≥–æ config)
    BRANDS = [
        {"name": "ACME", "url": "brands-by-acme~129973.html"},
        {"name": "Westwood Design", "url": "brands-by-westwood-design~937124.html"},
        {"name": "Legacy Classic", "url": "brands-by-legacy-classic~130027.html"},
        {"name": "Legacy Classic", "url": "brands-by-legacy-classic-kids~130028.html"},
        {"name": "Aspenhome Furniture", "url": "brands-by-aspenhome~129985.html"},
        {"name": "Steve Silver", "url": "brands-by-steve-silver~1242933.html"},
        {"name": "Intercon", "url": "brands-by-intercon~130018.html"},
    ]
    
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    ]
    
    def __init__(self, config: dict):
        self.config = config
        self.delay_min = config.get('delay_min', 2.0)
        self.delay_max = config.get('delay_max', 4.0)
        self.retry_attempts = config.get('retry_attempts', 3)
        self.timeout = config.get('timeout', 45)
        self.per_page = 40
        
        self.stats = {
            'total_products': 0,
            'unique_ids': 0,
            'duplicate_ids': 0,
            'brands_processed': 0,
            'pages_processed': 0,
            'errors': 0,
            'timeouts': 0
        }
        
        if not CURL_CFFI_AVAILABLE:
            logger.error("curl_cffi not available! Scraper may fail!")
        else:
            logger.info("‚úì curl_cffi available")
        
        logger.info("="*60)
        logger.info("Emma Mason Brands Scraper")
        logger.info("="*60)
    
    def _random_delay(self):
        """–ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏"""
        time.sleep(random.uniform(self.delay_min, self.delay_max))
    
    def _get_random_user_agent(self) -> str:
        """–í–∏–ø–∞–¥–∫–æ–≤–∏–π User-Agent"""
        return random.choice(self.USER_AGENTS)
    
    def _fetch_page(self, url: str, referer: Optional[str] = None) -> Optional[str]:
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É (curl_cffi + impersonate)
        """
        for attempt in range(1, self.retry_attempts + 1):
            try:
                headers = {
                    "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                    "accept-language": "en-US,en;q=0.9",
                    "accept-encoding": "gzip, deflate, br",
                    "user-agent": self._get_random_user_agent(),
                    "cache-control": "no-cache",
                    "pragma": "no-cache",
                }
                if referer:
                    headers["referer"] = referer
                
                if CURL_CFFI_AVAILABLE:
                    response = requests.get(
                        url,
                        headers=headers,
                        impersonate="chrome120",
                        timeout=self.timeout
                    )
                else:
                    response = requests.get(
                        url,
                        headers=headers,
                        timeout=self.timeout
                    )
                
                if response.status_code == 200:
                    return response.text
                
                elif response.status_code == 403:
                    logger.warning(f"403 Forbidden (attempt {attempt}/{self.retry_attempts})")
                    time.sleep(random.uniform(5, 10))
                
                else:
                    logger.warning(f"Status {response.status_code} (attempt {attempt})")
                    time.sleep(3)
                    
            except Exception as e:
                error_msg = str(e)
                
                if "timed out" in error_msg.lower() or "timeout" in error_msg.lower():
                    self.stats['timeouts'] += 1
                    logger.warning(f"Timeout (attempt {attempt}/{self.retry_attempts})")
                    time.sleep(random.uniform(5, 10))
                else:
                    logger.error(f"Request error (attempt {attempt}): {e}")
                    time.sleep(3)
        
        self.stats['errors'] += 1
        return None
    
    def _extract_products_from_page(self, html: str, brand_name: str) -> List[Dict]:
        """
        –í–∏—Ç—è–≥—Ç–∏ —Ç–æ–≤–∞—Ä–∏ –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –±—Ä–µ–Ω–¥—É
        
        ‚úÖ –ö–†–ò–¢–ò–ß–ù–û: –ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è batch_update_emma_mason():
        {
            'id': product_id,     # ‚Üê –ö–æ–ª–æ–Ω–∫–∞ R: ID from emmamason
            'url': url,           # ‚Üê –ö–æ–ª–æ–Ω–∫–∞ F: Our URL (–¥–ª—è –ø–æ—à—É–∫—É)
            'price': price,       # ‚Üê –ö–æ–ª–æ–Ω–∫–∞ D: Our Sales Price
            'brand': brand_name,  # –î–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        }
        """
        products = []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            product_items = soup.find_all('div', class_='product-item-info')
            
            for item in product_items:
                try:
                    # Product ID –∑ price-box data-product-id
                    price_box = item.find('div', {'data-role': 'priceBox'})
                    if not price_box:
                        continue
                    
                    product_id = price_box.get('data-product-id')
                    if not product_id:
                        continue
                    
                    # URL
                    link = item.find('a', class_='product-item-link')
                    url = link['href'] if link else None
                    
                    if not url:
                        continue
                    
                    # –¶—ñ–Ω–∞
                    price_elem = item.find('span', class_='price')
                    price = None
                    if price_elem:
                        price_text = price_elem.get_text(strip=True)
                        price = price_text.replace('$', '').replace(',', '').strip()
                        try:
                            float(price)
                        except:
                            price = None
                    
                    # ‚úÖ –°–¢–†–£–ö–¢–£–†–ê –¥–ª—è Google Sheets integration
                    # ‚úÖ –ö–õ–Æ–ß–û–í–û: –ü–æ–ª–µ 'id', –ù–ï 'product_id'!
                    products.append({
                        'id': product_id,           # ‚Üê ID from emmamason (R)
                        'url': url,                 # ‚Üê Our URL (F) –¥–ª—è –ø–æ—à—É–∫—É
                        'price': price,             # ‚Üê Our Sales Price (D)
                        'brand': brand_name,        # –î–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                        'scraped_at': datetime.now().isoformat()
                    })
                
                except Exception as e:
                    logger.debug(f"Failed to parse product: {e}")
                    continue
        
        except Exception as e:
            logger.error(f"Failed to parse page: {e}")
        
        return products
    
    def scrape_brand(self, brand_info: dict, seen_ids: Set[str]) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏—Ç–∏ –æ–¥–∏–Ω –±—Ä–µ–Ω–¥
        """
        brand_name = brand_info['name']
        brand_url = brand_info['url']
        
        logger.info(f"Processing brand: {brand_name}")
        
        brand_products = []
        page = 1
        
        while True:
            if page == 1:
                url = f"{self.BASE_URL}/{brand_url}?product_list_limit={self.per_page}"
            else:
                url = f"{self.BASE_URL}/{brand_url}?p={page}&product_list_limit={self.per_page}"
            
            logger.debug(f"  Page {page}: {url}")
            
            html = self._fetch_page(url, referer=self.BASE_URL)
            
            if not html:
                logger.error(f"  Failed to fetch page {page}")
                break
            
            if "Just a moment" in html or "Checking your browser" in html:
                logger.warning(f"  Cloudflare challenge detected")
                break
            
            products = self._extract_products_from_page(html, brand_name)
            
            if not products:
                logger.info(f"  No products on page {page}")
                break
            
            # –î–æ–¥–∞—Ç–∏ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ
            new_count = 0
            duplicate_count = 0
            
            for product in products:
                product_id = product['id']
                
                if product_id in seen_ids:
                    duplicate_count += 1
                    self.stats['duplicate_ids'] += 1
                else:
                    seen_ids.add(product_id)
                    brand_products.append(product)
                    new_count += 1
            
            logger.info(f"  Page {page}: {len(products)} products, {new_count} new, {duplicate_count} duplicates (total: {len(brand_products)})")
            
            self.stats['pages_processed'] += 1
            
            if len(products) < self.per_page:
                logger.info(f"  Last page (products < {self.per_page})")
                break
            
            if page >= 100:
                logger.warning(f"  Page limit reached (100)")
                break
            
            page += 1
            self._random_delay()
        
        logger.info(f"Brand {brand_name}: {len(brand_products)} unique products")
        self.stats['brands_processed'] += 1
        
        return brand_products
    
    def scrape_all_brands(self) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏—Ç–∏ –≤—Å—ñ –±—Ä–µ–Ω–¥–∏
        """
        logger.info("="*60)
        logger.info("Starting brand-based scraping")
        logger.info(f"Brands: {[b['name'] for b in self.BRANDS]}")
        logger.info("="*60)
        
        all_products = []
        seen_ids = set()
        start_time = datetime.now()
        
        for idx, brand_info in enumerate(self.BRANDS, 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"üìÇ BRAND {idx}/{len(self.BRANDS)}: {brand_info['name']}")
            logger.info(f"{'='*60}")
            
            products = self.scrape_brand(brand_info, seen_ids)
            all_products.extend(products)
            
            self.stats['total_products'] = len(all_products)
            self.stats['unique_ids'] = len(seen_ids)
            
            # –ü—Ä–æ–≥—Ä–µ—Å
            elapsed = (datetime.now() - start_time).total_seconds() / 60
            speed = len(all_products) / elapsed if elapsed > 0 else 0
            brands_left = len(self.BRANDS) - idx
            eta = (brands_left * elapsed / idx) if idx > 0 else 0
            
            logger.info(f"\n{'='*60}")
            logger.info(f"üìä OVERALL PROGRESS")
            logger.info(f"{'='*60}")
            logger.info(f"Brands: {idx}/{len(self.BRANDS)} ({idx/len(self.BRANDS)*100:.1f}%)")
            logger.info(f"Products: {len(all_products)} ({len(seen_ids)} unique IDs)")
            logger.info(f"Duplicates: {self.stats['duplicate_ids']}")
            logger.info(f"Speed: {speed:.1f} products/min")
            logger.info(f"Elapsed: {elapsed:.1f} min")
            logger.info(f"ETA: {eta:.1f} min")
            logger.info(f"Pages: {self.stats['pages_processed']}")
            logger.info(f"Errors: {self.stats['errors']}")
            logger.info(f"Timeouts: {self.stats['timeouts']}")
            logger.info(f"{'='*60}\n")
            
            if idx < len(self.BRANDS):
                time.sleep(3)
        
        duration = (datetime.now() - start_time).total_seconds() / 60
        
        logger.info("="*60)
        logger.info(f"‚úÖ COMPLETED")
        logger.info(f"Products: {len(all_products)} ({len(seen_ids)} unique IDs)")
        logger.info(f"Duplicates: {self.stats['duplicate_ids']}")
        logger.info(f"Brands: {self.stats['brands_processed']}")
        logger.info(f"Pages: {self.stats['pages_processed']}")
        logger.info(f"Time: {duration:.1f} minutes")
        logger.info(f"Speed: {len(all_products)/duration:.1f} products/min")
        logger.info(f"Errors: {self.stats['errors']}")
        logger.info(f"Timeouts: {self.stats['timeouts']}")
        logger.info("="*60)
        
        return all_products
    
    def get_stats(self) -> dict:
        """–ü–æ–≤–µ—Ä–Ω—É—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        return self.stats.copy()


def scrape_emmamason_brands(config: dict) -> List[Dict]:
    """
    –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É Emma Mason
    
    ‚úÖ –í–ò–ö–û–†–ò–°–¢–û–í–£–Ñ–¢–¨–°–Ø –í app/main.py:
    from app.scrapers.emmamason_brands import scrape_emmamason_brands
    results = scrape_emmamason_brands(config)
    
    Returns:
        List[Dict]: [{id, url, price, brand}, ...]
    """
    scraper = EmmaMasonBrandsScraper(config)
    results = scraper.scrape_all_brands()
    return results


if __name__ == "__main__":
    # –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(levelname)-8s | %(message)s',
        datefmt='%H:%M:%S'
    )
    
    if not CURL_CFFI_AVAILABLE:
        print("\n‚ö†Ô∏è Warning: curl_cffi not installed!")
        print("Install: pip install curl-cffi")
        print("Continuing with standard requests (may fail)\n")
    
    config = {
        'delay_min': 2.0,
        'delay_max': 4.0,
        'retry_attempts': 3,
        'timeout': 45
    }
    
    print("\n" + "="*60)
    print("–¢–ï–°–¢ EMMA MASON BRANDS SCRAPER")
    print("="*60 + "\n")
    
    results = scrape_emmamason_brands(config)
    
    print("\n" + "="*60)
    print(f"–†–ï–ó–£–õ–¨–¢–ê–¢: {len(results)} products")
    print("="*60)
    
    if results:
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –±—Ä–µ–Ω–¥–∞—Ö
        brands = {}
        for p in results:
            brand = p['brand']
            brands[brand] = brands.get(brand, 0) + 1
        
        print("\nBy brand:")
        for brand, count in brands.items():
            print(f"  {brand}: {count}")
        
        print(f"\nSample (first 3):")
        for i, p in enumerate(results[:3], 1):
            print(f"\n{i}. ID: {p['id']}")
            print(f"   Brand: {p['brand']}")
            print(f"   Price: ${p.get('price', 'N/A')}")
            print(f"   URL: {p['url'][:60]}...")
    else:
        print("\n‚ùå No products scraped")
    
    print("\n" + "="*60)
