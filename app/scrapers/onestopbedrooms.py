"""
1StopBedrooms Scraper - BRAND-BASED VERSION (OPTIMIZED!)
–ü–∞—Ä—Å–∏—Ç—å —Ü—ñ–Ω–∏ –∑ 1stopbedrooms.com —á–µ—Ä–µ–∑ GraphQL API –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –≤–∏—Ä–æ–±–Ω–∏–∫–∞—Ö
–ù–ê–ë–ê–ì–ê–¢–û –®–í–ò–î–®–ï –Ω—ñ–∂ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö - ~5-10 —Ö–≤–∏–ª–∏–Ω –∑–∞–º—ñ—Å—Ç—å 30-40!
"""

import requests
import time
import logging
from typing import List, Dict, Optional
from datetime import datetime

logger = logging.getLogger("onestopbedrooms")


class OneStopBedroomsScraper:
    """Scraper –¥–ª—è 1stopbedrooms.com - –∑–±–∏—Ä–∞—î –ø–æ –≤–∏—Ä–æ–±–Ω–∏–∫–∞—Ö"""
    
    BASE_URL = "https://www.1stopbedrooms.com"
    GRAPHQL_URL = "https://www.1stopbedrooms.com/graphql/1"
    
    # ‚úÖ –¶—ñ–ª—å–æ–≤—ñ –≤–∏—Ä–æ–±–Ω–∏–∫–∏ (—Ç—ñ —Å–∞–º—ñ —â–æ –≤ Coleman —ñ AFA)
    BRANDS = [
        {"name": "Steve Silver", "slug": "brand/steve-silver"},
        {"name": "Martin Furniture", "slug": "brand/martin-furniture"},
        {"name": "Legacy Classic", "slug": "brand/legacy-classic"},
        {"name": "Aspenhome", "slug": "brand/aspenhome"},
        {"name": "ACME", "slug": "brand/acme"},
    ]
    
    GRAPHQL_QUERY = """
fragment fragmentImage on catalogSearchImage {
    style
    alt
    src
    sources { media srcset }
    classes
}
fragment fragmentDynamicAttribute on catalogSearchPrimaryDynamicAttributeInterface {
    __typename
    type
    ... on catalogSearchPrimaryDynamicAttributeTextType { count label }
    ... on catalogSearchPrimaryDynamicAttributeThumbnailType { additional items { image webId classes } }
    ... on catalogSearchPrimaryDynamicAttributeSwatchThumbnailType { additional items { image webId classes } }
}
fragment fragmentCatalogProduct on catalogSearchProductInterface {
    __typename
    id
    webId
    name
    labels
    outOfStock
    brand { id name }
    cat_names
    collection
    sku
    useInCriteoFeed
    slug
    url
    shippingType
    price { finalPrice msrp showMsrp getExcludePromo getSale }
    tags {
        allTags
        primaryDealTag
        primaryDealTagVisibility
        secondaryTag
        secondaryTagVisibility
        tertiaryTag
        tertiaryTagVisibility
        __typename
    }
    couponData { discount code }
    images { mainImage { ...fragmentImage } }
    reviews { number rating }
    mattressData { comfort height type top_type label }
    ... on catalogSearchProductDynamicItem {
        dynamicAttribute { ...fragmentDynamicAttribute }
        secondaryDynamicAttribute { ...fragmentDynamicAttribute }
    }
    freewg
}
query getListingData($slug: String!, $request: catalogSearchFilterInput, $zipcode: String) {
    listing {
        listingCategory(slug: $slug, request: $request, zipcode: $zipcode) {
            itemsCount
            perPage
            pages
            page
            header: title
            items {
                ...fragmentCatalogProduct
                ... on catalogSearchProductDynamicItem { items { ...fragmentCatalogProduct } }
            }
        }
        topSellerProducts(categorySlug: $slug) {
            webId
            title
            brand { name }
            url
            slug
            labels
            outOfStock
            price { price regularPrice finalPrice msrp showMsrp }
            reviews { number rating }
            images {
                mainImage {
                    style
                    alt
                    src
                    sources { media srcset }
                    classes
                }
            }
        }
    }
}
"""
    
    def __init__(self, config: dict):
        self.config = config
        self.delay_min = config.get('delay_min', 1.0)
        self.delay_max = config.get('delay_max', 3.0)
        self.retry_attempts = config.get('retry_attempts', 3)
        self.timeout = config.get('timeout', 20)
        
        self.stats = {
            'total_products': 0,
            'unique_products': 0,
            'errors': 0,
            'brands_processed': 0
        }
        
        logger.info("1StopBedrooms scraper initialized (brand-based - FAST!)")
    
    def _random_delay(self):
        """–ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏"""
        import random
        time.sleep(random.uniform(self.delay_min, self.delay_max))
    
    def _safe_request(self, payload: dict, headers: dict) -> Optional[dict]:
        """–í–∏–∫–æ–Ω—É—î GraphQL –∑–∞–ø–∏—Ç –∑ retry –ª–æ–≥—ñ–∫–æ—é"""
        for attempt in range(self.retry_attempts):
            try:
                response = requests.post(
                    self.GRAPHQL_URL,
                    headers=headers,
                    json=payload,
                    timeout=self.timeout
                )
                response.raise_for_status()
                return response.json()
            except Exception as e:
                logger.warning(f"Request error (attempt {attempt+1}/{self.retry_attempts}): {e}")
                if attempt < self.retry_attempts - 1:
                    time.sleep(5)
        
        self.stats['errors'] += 1
        return None
    
    def _extract_products(self, items: List[dict], seen_skus: set) -> List[Dict[str, str]]:
        """–í–∏—Ç—è–≥—É—î —Ç–æ–≤–∞—Ä–∏ –∑—ñ —Å–ø–∏—Å–∫—É items"""
        products = []
        
        for item in items:
            typename = item.get("__typename")
            
            # –í–∏–∑–Ω–∞—á–∞—î–º–æ —Ç–∏–ø —Ç–æ–≤–∞—Ä—É
            if typename == "catalogSearchProductSimpleItem":
                sub_items = [item]
            elif typename == "catalogSearchProductDynamicItem":
                sub_items = item.get("items") or [item]
            elif typename == "catalogSearchProductBundleItem":
                sub_items = [item]
            else:
                logger.debug(f"Unknown product type: {typename}")
                sub_items = []
            
            # –û–±—Ä–æ–±–ª—è—î–º–æ –∫–æ–∂–µ–Ω –ø—ñ–¥-—Ç–æ–≤–∞—Ä
            for prod in sub_items:
                if not prod:
                    continue
                
                sku = prod.get("sku")
                if not sku or sku in seen_skus:
                    continue
                
                seen_skus.add(sku)
                
                # –í–∏—Ç—è–≥—É—î–º–æ –¥–∞–Ω—ñ
                brand = prod.get("brand", {})
                price_data = prod.get("price", {})
                
                products.append({
                    "sku": sku,
                    "brand": brand.get("name") if brand else None,
                    "price": price_data.get("finalPrice"),
                    "url": prod.get("url")
                })
        
        return products
    
    def scrape_brand(self, brand_info: dict, seen_skus: set) -> List[Dict[str, str]]:
        """–ü–∞—Ä—Å–∏—Ç—å –≤—Å—ñ —Ç–æ–≤–∞—Ä–∏ –æ–¥–Ω–æ–≥–æ –≤–∏—Ä–æ–±–Ω–∏–∫–∞"""
        brand_name = brand_info["name"]
        brand_slug = brand_info["slug"]
        
        logger.info(f"Processing brand: {brand_name} ({brand_slug})")
        
        brand_products = []
        page = 1
        
        headers = {
            "Content-Type": "application/json",
            "Accept": "*/*",
            "Origin": self.BASE_URL,
            "Referer": f"{self.BASE_URL}/{brand_slug}",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        
        # –ü–µ—Ä—à–∏–π –∑–∞–ø–∏—Ç —â–æ–± –¥—ñ–∑–Ω–∞—Ç–∏—Å—å –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ç–æ—Ä—ñ–Ω–æ–∫
        payload = {
            "operationName": "getListingData",
            "query": self.GRAPHQL_QUERY,
            "variables": {
                "slug": brand_slug,
                "request": {
                    "sortBy": "RELEVANCE",
                    "perPage": "PER_PAGE_48",
                    "page": 1,
                    "facet": []
                },
                "zipcode": "11229"  # –ú–æ–∂–Ω–∞ –∑–∞–ª–∏—à–∏—Ç–∏ –∞–±–æ –∑–º—ñ–Ω–∏—Ç–∏
            }
        }
        
        data = self._safe_request(payload, headers)
        if not data or "data" not in data:
            logger.error(f"Failed to get data for brand {brand_name}")
            return []
        
        try:
            listing = data["data"]["listing"]["listingCategory"]
            max_page = listing["pages"]
            items_count = listing["itemsCount"]
            
            logger.info(f"Brand {brand_name}: {items_count} items, {max_page} pages")
            
            # –í–∏—Ç—è–≥—Ç–∏ —Ç–æ–≤–∞—Ä–∏ –∑ –ø–µ—Ä—à–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏
            items = listing.get("items", [])
            products = self._extract_products(items, seen_skus)
            brand_products.extend(products)
            
            logger.info(f"  Page 1/{max_page}: found {len(products)} new products (total: {len(brand_products)})")
            
        except KeyError as e:
            logger.error(f"Missing data in response: {e}")
            return []
        
        # –ü–∞—Ä—Å–∏–º–æ —Ä–µ—à—Ç—É —Å—Ç–æ—Ä—ñ–Ω–æ–∫
        for page in range(2, max_page + 1):
            payload["variables"]["request"]["page"] = page
            
            data = self._safe_request(payload, headers)
            if not data:
                logger.warning(f"Failed to load page {page}, skipping...")
                continue
            
            try:
                items = data["data"]["listing"]["listingCategory"]["items"]
                products = self._extract_products(items, seen_skus)
                brand_products.extend(products)
                
                logger.info(f"  Page {page}/{max_page}: found {len(products)} new products (total: {len(brand_products)})")
                
            except KeyError as e:
                logger.error(f"Missing data on page {page}: {e}")
                continue
            
            # –ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ —Å—Ç–æ—Ä—ñ–Ω–∫–∞–º–∏
            if page < max_page:
                self._random_delay()
        
        logger.info(f"Brand {brand_name}: collected {len(brand_products)} unique products")
        self.stats['brands_processed'] += 1
        
        return brand_products
    
    def scrape_all_products(self) -> List[Dict[str, str]]:
        """–ü–∞—Ä—Å–∏—Ç—å –≤—Å—ñ —Ç–æ–≤–∞—Ä–∏ –≤—ñ–¥ —Ü—ñ–ª—å–æ–≤–∏—Ö –≤–∏—Ä–æ–±–Ω–∏–∫—ñ–≤"""
        logger.info("="*60)
        logger.info("Starting 1StopBedrooms scraping (brand-based)")
        logger.info(f"Target brands: {[b['name'] for b in self.BRANDS]}")
        logger.info("="*60)
        
        all_products = []
        seen_skus = set()
        start_time = datetime.now()
        
        for idx, brand_info in enumerate(self.BRANDS, 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"üìÇ BRAND {idx}/{len(self.BRANDS)}: {brand_info['name']}")
            logger.info(f"{'='*60}")
            
            products = self.scrape_brand(brand_info, seen_skus)
            all_products.extend(products)
            
            # –ó–∞–≥–∞–ª—å–Ω–∏–π –ø—Ä–æ–≥—Ä–µ—Å –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ–≥–æ –≤–∏—Ä–æ–±–Ω–∏–∫–∞
            elapsed = (datetime.now() - start_time).total_seconds() / 60
            speed = len(all_products) / elapsed if elapsed > 0 else 0
            brands_left = len(self.BRANDS) - idx
            eta = (brands_left * elapsed / idx) if idx > 0 else 0
            
            logger.info(f"\n{'='*60}")
            logger.info(f"üìä OVERALL PROGRESS")
            logger.info(f"{'='*60}")
            logger.info(f"Brands: {idx}/{len(self.BRANDS)} ({idx/len(self.BRANDS)*100:.1f}%)")
            logger.info(f"Total products: {len(all_products)} ({len(seen_skus)} unique)")
            logger.info(f"Speed: {speed:.1f} products/min")
            logger.info(f"Elapsed: {elapsed:.1f} min")
            logger.info(f"ETA: {eta:.1f} min")
            logger.info(f"Errors: {self.stats['errors']}")
            logger.info(f"{'='*60}\n")
            
            self.stats['total_products'] = len(all_products)
            self.stats['unique_products'] = len(seen_skus)
            
            # –ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –≤–∏—Ä–æ–±–Ω–∏–∫–∞–º–∏
            time.sleep(2)
        
        logger.info("="*60)
        logger.info(f"‚úÖ COMPLETED: {len(all_products)} products from {len(seen_skus)} unique SKUs")
        logger.info(f"Brands processed: {self.stats['brands_processed']}")
        logger.info(f"Errors: {self.stats['errors']}")
        logger.info(f"Total time: {(datetime.now() - start_time).total_seconds() / 60:.1f} minutes")
        logger.info("="*60)
        
        return all_products
    
    def get_stats(self) -> dict:
        """–ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        return self.stats.copy()


def scrape_onestopbedrooms(config: dict) -> List[Dict[str, str]]:
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É 1StopBedrooms"""
    scraper = OneStopBedroomsScraper(config)
    results = scraper.scrape_all_products()
    return results


if __name__ == "__main__":
    # –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(levelname)-8s | %(message)s',
        datefmt='%H:%M:%S'
    )
    
    test_config = {
        'delay_min': 1.0,
        'delay_max': 2.0,
        'retry_attempts': 3,
        'timeout': 20
    }
    
    print("\n" + "="*60)
    print("–¢–ï–°–¢ 1STOPBEDROOMS SCRAPER (BRAND-BASED - FAST!)")
    print("="*60 + "\n")
    
    results = scrape_onestopbedrooms(test_config)
    
    print("\n" + "="*60)
    print(f"–†–ï–ó–£–õ–¨–¢–ê–¢: {len(results)} —Ç–æ–≤–∞—Ä—ñ–≤")
    print("="*60)
    
    if results:
        # –ü–æ–∫–∞–∑–∞—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤–∏—Ä–æ–±–Ω–∏–∫–∞—Ö
        brands = {}
        for product in results:
            brand = product['brand']
            if brand:
                brands[brand] = brands.get(brand, 0) + 1
        
        print("\n–ü–æ –≤–∏—Ä–æ–±–Ω–∏–∫–∞—Ö:")
        for brand, count in sorted(brands.items()):
            print(f"  {brand}: {count} —Ç–æ–≤–∞—Ä—ñ–≤")
        
        print("\n–ü–µ—Ä—à—ñ 5 —Ç–æ–≤–∞—Ä—ñ–≤:")
        for i, product in enumerate(results[:5], 1):
            print(f"\n{i}. SKU: {product['sku']}")
            print(f"   Brand: {product['brand']}")
            print(f"   Price: ${product['price']}")
            print(f"   URL: {product['url'][:60]}...")
    else:
        print("\n‚ùå –ù–µ–º–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤")
    
    print("\n" + "="*60)
