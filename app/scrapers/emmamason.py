"""
Emma Mason Production Scraper - CURL_CFFI APPROACH (WORKING!)
–ë–∞–∑—É—î—Ç—å—Å—è –Ω–∞ –û–†–ò–ì–Ü–ù–ê–õ–¨–ù–û–ú–£ —Ä–æ–±–æ—á–æ–º—É –∫–æ–¥—ñ –∑ curl_cffi
–ë–ï–ó Selenium - –ø—Ä–æ—Å—Ç—ñ—à–µ, —à–≤–∏–¥—à–µ, –º–µ–Ω—à–µ —Ä–µ—Å—É—Ä—Å—ñ–≤!
"""

import json
import time
import random
from typing import List, Dict, Optional
from datetime import datetime
from bs4 import BeautifulSoup
import logging

# –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ curl_cffi (–û–ë–û–í'–Ø–ó–ö–û–í–û –¥–ª—è production!)
try:
    from curl_cffi import requests
    CURL_CFFI_AVAILABLE = True
except ImportError:
    import requests
    CURL_CFFI_AVAILABLE = False
    print("‚ùå curl_cffi not found! Install: pip install curl-cffi")
    print("‚ö†Ô∏è  Fallback to standard requests (may fail with 403)")

logger = logging.getLogger("emmamason_production")


class EmmaMasonProductionScraper:
    """Production scraper –¥–ª—è emmamason.com - curl_cffi –ø—ñ–¥—Ö—ñ–¥"""
    
    BASE_URL = "https://emmamason.com"
    
    # 5 —Ü—ñ–ª—å–æ–≤–∏—Ö –≤–∏—Ä–æ–±–Ω–∏–∫—ñ–≤
    BRANDS = [
        {"name": "ACME", "url": "brands-acme-furniture.html"},
        {"name": "Westwood Design", "url": "brands-by-westwood-design~937124.html.html"},
        {"name": "Legacy Classic", "url": "brands-legacy-classic.html"},
        {"name": "Aspenhome Furniture", "url": "aspenhome-furniture-by-aspenhome~587712.html.html"},
        {"name": "Steve Silver", "url": "steve-silver-by-steve-silver~1804527.html.html"},
        {"name": "Intercon", "url": "intercon-furniture-by-intercon-furniture~1035926.html"},
    ]
    
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    ]
    
    def __init__(self, config: dict):
        self.config = config
        self.delay_min = config.get('delay_min', 2.0)
        self.delay_max = config.get('delay_max', 4.0)
        self.retry_attempts = config.get('retry_attempts', 3)
        self.timeout = config.get('timeout', 30)
        self.per_page = 40  # Products per page
        
        self.stats = {
            'total_products': 0,
            'unique_products': 0,
            'brands_processed': 0,
            'pages_processed': 0,
            'errors': 0
        }
        
        if not CURL_CFFI_AVAILABLE:
            logger.error("curl_cffi not available! Scraper may fail!")
        else:
            logger.info("‚úì curl_cffi available - Cloudflare bypass ready")
        
        logger.info("="*60)
        logger.info("Emma Mason Production Scraper (curl_cffi)")
        logger.info(f"Method: HTTP requests (no browser)")
        logger.info(f"Cloudflare bypass: impersonate=chrome120")
        logger.info("="*60)
    
    def _random_delay(self):
        """–ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏"""
        time.sleep(random.uniform(self.delay_min, self.delay_max))
    
    def _get_random_user_agent(self) -> str:
        """–í–∏–ø–∞–¥–∫–æ–≤–∏–π User-Agent"""
        return random.choice(self.USER_AGENTS)
    
    def _fetch_page(self, url: str, referer: Optional[str] = None) -> Optional[str]:
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É (–ö–õ–Æ–ß–û–í–ò–ô –ú–ï–¢–û–î!)
        –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î curl_cffi + impersonate –¥–ª—è –æ–±—Ö–æ–¥—É Cloudflare
        """
        for attempt in range(1, self.retry_attempts + 1):
            try:
                headers = {
                    "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                    "accept-language": "en-US,en;q=0.9",
                    "accept-encoding": "gzip, deflate, br",
                    "user-agent": self._get_random_user_agent(),
                    "cache-control": "no-cache",
                    "pragma": "no-cache",
                }
                if referer:
                    headers["referer"] = referer
                
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                # –ö–õ–Æ–ß –î–û –£–°–ü–Ü–•–£: curl_cffi + impersonate="chrome120"
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                if CURL_CFFI_AVAILABLE:
                    response = requests.get(
                        url,
                        headers=headers,
                        impersonate="chrome120",  # ‚Üê –ö–†–ò–¢–ò–ß–ù–û!
                        timeout=self.timeout
                    )
                else:
                    # Fallback (–º–æ–∂–µ –Ω–µ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏)
                    response = requests.get(
                        url,
                        headers=headers,
                        timeout=self.timeout
                    )
                
                if response.status_code == 200:
                    return response.text
                
                elif response.status_code == 403:
                    logger.warning(f"403 Forbidden (attempt {attempt}/{self.retry_attempts})")
                    time.sleep(random.uniform(5, 10))
                
                else:
                    logger.warning(f"Status {response.status_code} (attempt {attempt})")
                    time.sleep(3)
                    
            except Exception as e:
                logger.error(f"Request error (attempt {attempt}): {e}")
                time.sleep(3)
        
        self.stats['errors'] += 1
        return None
    
    def _extract_products_from_page(self, html: str, brand_name: str) -> List[Dict]:
        """
        –í–∏—Ç—è–≥—Ç–∏ —Ç–æ–≤–∞—Ä–∏ –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –±—Ä–µ–Ω–¥—É
        –ü–∞—Ä—Å–∏—Ç—å product-item-info –±–ª–æ–∫–∏
        –ó–±–∏—Ä–∞—î: product_id, price, url
        """
        products = []

        try:
            soup = BeautifulSoup(html, 'html.parser')

            # –ó–Ω–∞–π—Ç–∏ –≤—Å—ñ product items
            product_items = soup.find_all('div', class_='product-item-info')

            for item in product_items:
                try:
                    # Product ID (–∑ price-box data-product-id)
                    price_box = item.find('div', {'data-role': 'priceBox'})
                    if not price_box:
                        logger.debug("No price-box found, skipping")
                        continue

                    product_id = price_box.get('data-product-id')
                    if not product_id:
                        logger.debug("No product ID found, skipping")
                        continue

                    # URL
                    link = item.find('a', class_='product-item-link')
                    if not link:
                        logger.debug("No link found, skipping")
                        continue
                    url = link.get('href')
                    if not url:
                        logger.debug("No href found, skipping")
                        continue

                    # –¶—ñ–Ω–∞
                    price_elem = item.find('span', class_='price')
                    price = None
                    if price_elem:
                        price_text = price_elem.get_text(strip=True)
                        price = price_text.replace('$', '').replace(',', '').strip()
                        try:
                            float(price)  # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —â–æ –≤–∞–ª—ñ–¥–Ω–∞
                        except:
                            price = None

                    products.append({
                        'product_id': product_id,
                        'brand': brand_name,
                        'url': url,
                        'price': price,
                        'scraped_at': datetime.now().isoformat()
                    })

                except Exception as e:
                    logger.debug(f"Failed to parse product item: {e}")
                    continue

        except Exception as e:
            logger.error(f"Failed to parse page HTML: {e}")

        return products
    
    def scrape_brand(self, brand_info: dict, seen_ids: set) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏—Ç–∏ –æ–¥–∏–Ω –±—Ä–µ–Ω–¥ (–≤—Å—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏)
        """
        brand_name = brand_info['name']
        brand_url = brand_info['url']

        logger.info(f"Processing brand: {brand_name}")

        brand_products = []
        page = 1

        while True:
            # URL –∑ pagination
            if page == 1:
                url = f"{self.BASE_URL}/{brand_url}?product_list_limit={self.per_page}"
            else:
                url = f"{self.BASE_URL}/{brand_url}?p={page}&product_list_limit={self.per_page}"

            logger.debug(f"  Page {page}: {url}")

            # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É
            html = self._fetch_page(url, referer=self.BASE_URL)

            if not html:
                logger.error(f"  Failed to fetch page {page}")
                break

            # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ Cloudflare challenge
            if "Just a moment" in html or "Checking your browser" in html:
                logger.warning(f"  Cloudflare challenge detected!")
                logger.warning(f"  This should not happen with curl_cffi + impersonate")
                break

            # –í–∏—Ç—è–≥—Ç–∏ —Ç–æ–≤–∞—Ä–∏
            products = self._extract_products_from_page(html, brand_name)

            if not products:
                logger.info(f"  No products on page {page}, end of brand")
                break

            # –î–æ–¥–∞—Ç–∏ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ
            new_count = 0
            for product in products:
                product_id = product['product_id']
                if product_id not in seen_ids:
                    seen_ids.add(product_id)
                    brand_products.append(product)
                    new_count += 1

            logger.info(f"  Page {page}: {len(products)} products, {new_count} new (total: {len(brand_products)})")

            self.stats['pages_processed'] += 1

            # –û—Å—Ç–∞–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∞?
            if len(products) < self.per_page:
                logger.info(f"  Last page (products < {self.per_page})")
                break

            # –õ—ñ–º—ñ—Ç —Å—Ç–æ—Ä—ñ–Ω–æ–∫
            if page >= 50:
                logger.warning(f"  Page limit reached (50)")
                break

            page += 1

            # –ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ —Å—Ç–æ—Ä—ñ–Ω–∫–∞–º–∏
            self._random_delay()

        logger.info(f"Brand {brand_name}: {len(brand_products)} unique products")
        self.stats['brands_processed'] += 1

        return brand_products
    
    def scrape_all_products(self) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏—Ç–∏ –≤—Å—ñ –±—Ä–µ–Ω–¥–∏
        –ì–æ–ª–æ–≤–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è production
        """
        logger.info("="*60)
        logger.info("Starting brand-based scraping")
        logger.info(f"Brands: {[b['name'] for b in self.BRANDS]}")
        logger.info("="*60)

        all_products = []
        seen_ids = set()
        start_time = datetime.now()

        for idx, brand_info in enumerate(self.BRANDS, 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"üìÇ BRAND {idx}/{len(self.BRANDS)}: {brand_info['name']}")
            logger.info(f"{'='*60}")

            products = self.scrape_brand(brand_info, seen_ids)
            all_products.extend(products)

            self.stats['total_products'] = len(all_products)
            self.stats['unique_products'] = len(seen_ids)

            # –ü—Ä–æ–≥—Ä–µ—Å
            elapsed = (datetime.now() - start_time).total_seconds() / 60
            speed = len(all_products) / elapsed if elapsed > 0 else 0
            brands_left = len(self.BRANDS) - idx
            eta = (brands_left * elapsed / idx) if idx > 0 else 0

            logger.info(f"\n{'='*60}")
            logger.info(f"üìä OVERALL PROGRESS")
            logger.info(f"{'='*60}")
            logger.info(f"Brands: {idx}/{len(self.BRANDS)} ({idx/len(self.BRANDS)*100:.1f}%)")
            logger.info(f"Products: {len(all_products)} ({len(seen_ids)} unique)")
            logger.info(f"Speed: {speed:.1f} products/min")
            logger.info(f"Elapsed: {elapsed:.1f} min")
            logger.info(f"ETA: {eta:.1f} min")
            logger.info(f"Pages: {self.stats['pages_processed']}")
            logger.info(f"Errors: {self.stats['errors']}")
            logger.info(f"{'='*60}\n")

            # –ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –±—Ä–µ–Ω–¥–∞–º–∏
            if idx < len(self.BRANDS):
                time.sleep(3)

        duration = (datetime.now() - start_time).total_seconds() / 60

        logger.info("="*60)
        logger.info(f"‚úÖ COMPLETED")
        logger.info(f"Products: {len(all_products)} ({len(seen_ids)} unique IDs)")
        logger.info(f"Brands: {self.stats['brands_processed']}")
        logger.info(f"Pages: {self.stats['pages_processed']}")
        logger.info(f"Time: {duration:.1f} minutes")
        logger.info(f"Speed: {len(all_products)/duration:.1f} products/min")
        logger.info(f"Errors: {self.stats['errors']}")
        logger.info("="*60)

        return all_products
    
    def get_stats(self) -> dict:
        """–ü–æ–≤–µ—Ä–Ω—É—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        return self.stats.copy()


def scrape_emmamason_production(config: dict) -> List[Dict]:
    """
    –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è production
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î curl_cffi –ø—ñ–¥—Ö—ñ–¥ (–ë–ï–ó Selenium!)
    """
    scraper = EmmaMasonProductionScraper(config)
    results = scraper.scrape_all_products()
    return results


if __name__ == "__main__":
    # –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(levelname)-8s | %(message)s',
        datefmt='%H:%M:%S'
    )
    
    if not CURL_CFFI_AVAILABLE:
        print("\n" + "="*60)
        print("‚ùå curl_cffi NOT INSTALLED!")
        print("="*60)
        print("\nInstall it:")
        print("  pip install curl-cffi")
        print("\nWithout curl_cffi, scraper will fail with 403 Forbidden!")
        print("="*60 + "\n")
        exit(1)
    
    # Production config
    config = {
        'delay_min': 2.0,
        'delay_max': 4.0,
        'retry_attempts': 3,
        'timeout': 30
    }
    
    print("\n" + "="*60)
    print("PRODUCTION SCRAPER TEST (curl_cffi approach)")
    print("="*60 + "\n")
    
    results = scrape_emmamason_production(config)
    
    print("\n" + "="*60)
    print(f"RESULT: {len(results)} products")
    print("="*60)
    
    if results:
        # –ó–±–µ—Ä–µ–≥—Ç–∏
        output_file = f"emmamason_production_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\n‚úì Saved: {output_file}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        brands = {}
        for p in results:
            brand = p['brand']
            brands[brand] = brands.get(brand, 0) + 1
        
        print("\nBy brand:")
        for brand, count in brands.items():
            print(f"  {brand}: {count}")
        
        print(f"\nSample (first 3):")
        for i, p in enumerate(results[:3], 1):
            print(f"\n{i}. SKU: {p['sku']}")
            print(f"   Brand: {p['brand']}")
            print(f"   Price: ${p.get('price', 'N/A')}")
            print(f"   URL: {p['url'][:60]}...")
    else:
        print("\n‚ùå No products scraped")
    
    print("\n" + "="*60)
